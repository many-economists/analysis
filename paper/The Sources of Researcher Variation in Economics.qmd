---
title: "The Sources of Researcher Variation in Economics"
author:
  - id: HK1
    number: 1
    name: Nick Huntington-Klein\thanks{Corresponding author Nick Huntington-Klein, nhuntington-klein@seattleu.edu, +1 (206) 296-5815. Department of Economics, Seattle University, 901 12th ave., Seattle, WA, 98122. Huntington-Klein and Pörtner are the project organizers. This project was supported by the Alfred P. Sloan foundation grant G-2022-19377. The Seattle University IRB determined this study to be exempt from IRB review in accordance with federal regulation criteria. Many thanks to Kian Farzaneh, Amrapali Samanta, and Erica Long for research assistance, to the researchers Mira Chaskes, Jennifer A. Heissel, Elaine L. Hill, Rajius Idzalika, Joshua D. Merfeld, and Ethan Sawyer, who contributed but did not want an authorship slot, to the researchers who wished to remain anonymous, and to the researchers who enlisted in the study but were not eligible or were not able to complete all three rounds of the project.}
    email: nhuntington-klein@seattleu.edu
    phone: 1-206-296-5815
    fax: NONE
    orcid: 0000-0002-7352-3991
    degrees: PhD
    attributes:
      corresponding: True
    affiliations:
      - id: NHK1_1
        number: 1
        name: Seattle University
        department: Department of Economics
        address: 901 12th Ave, Seattle, WA, 98122
  - id: CP2
    number: 2
    name: Claus Pörtner
    email: cportner@seattleu.edu
    phone: 1-206-296-2593
    fax: NONE
    orcid: 0000-0001-8052-9462
    degrees: PhD
    attributes:
      corresponding: True
    affiliations:
      - id: CP1_1
        number: 1
        name: Seattle University
        department: Department of Economics
        address: 901 12th Ave, Seattle, WA, 98122
      - id: CP1_2
        number: 2
        name: Center for Studies in Demography and Ecology 
        address: University of Washington, Seattle, WA, 98195
format: 
  pdf:
    keep-tex: true
editor: source
bibliography: References.bib
number-sections: true
execute:
  warning: false
  message: false
  echo: false
abstract: We have 146 research teams of economists independently use the same data source to answer the same question about the causal effect of a policy. Each team performs the task three times, first with free choice of how to answer the question, second with all researchers required to use a shared research design, and third with pre-cleaned data and a shared research design. We find considerable variation across researchers in reported sample sizes, sample definitions, and modeling choices, although estimated effects show muted variation. Levels of researcher variation were most heavily influenced by data preparation and cleaning, with much smaller roles for researcher background, modeling and estimation decisions, and exposure to peer review. This implies that data preparation and cleaning should receive considerably more attention in researcher training and the evaluation of research.
include-in-header:
  text:
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{placeins}
output: 
  rticles::arxiv_article:
    keep_tex: true
---

```{r}
{
  library(rio)
  library(data.table)
  library(ggplot2)
  library(nicksshorts) # remotes::install_github('NickCH-K/nicksshorts')
  library(stringr)
  library(scales)
  library(vtable)
  library(fixest)
  library(modelsummary)
  library(here)
}

dat = import("../data/cleaned_survey_post_corrections.parquet", setclass = 'data.table')
dat[, Revision_of_Q14 := str_replace_all(Revision_of_Q14, '‚Äì','-')]
dat[, Revision_of_Q17 := str_replace_all(Revision_of_Q17, '‚Äì','-')]
dat[, Revision_of_Q20 := str_replace_all(Revision_of_Q20, '‚Äì','-')]

# CHANGE THIS COLOR PALETTE TO CHANGE ALL GRAPHS
colorpal = palette.colors(palette = 'Paired')
```

# Introduction

The social and behavioral sciences produce a staggering amount of empirical results. A responsible reader of this literature should wonder how much they can trust a given study, given the potential for errors, fluke results, or intentional manipulation. Furthermore, as any responsible producer of this literature is well aware, even a researcher doing their best to avoid these problems must make hundreds of choices in the process of collecting and cleaning data, planning their estimation, and coding their analysis---in other words, there are numerous "researcher degrees of freedom" [@simmons2011false]. Even if Researcher A's choices stand up to scrutiny from a reviewer or reader, Researcher B---with the same goal, data, and skills---might have reasonably chosen in a different way that would also stand up to scrutiny. If A and B's choices lead to different results, but only one of them performs the study, this is a source of largely arbitrary variation in the collection of published results. Estimates suggest that, at least in one context, this variation might outweigh the population variation we typically consider when estimating standard errors [@holzmeister2023heterogeneity].

This study examines the impact of researcher degrees of freedom on results, and attempts to isolate researcher degrees of freedom at different stages of the research process to try to determine where researcher choice varies most, and where it most strongly influences results. We do this using a "many-analysts" design where multiple researchers attempt the same research task. The task we chose is common across applied econometrics: estimating the causal effect of a policy that is implemented at a specific period of time and affects some people but not others. We look at differences between researchers in the results as well as in their analytic and data cleaning choices.

We expand on a typical many-analysts design by introducing multiple iterations of the task, each time restricting the amount of choice that researchers can make and so reducing researcher degrees of freedom. This allows us to observe the overall amount of variation in estimates between researchers, as is common in many-analysts designs, and also to separately evaluate the influence of choice in research design and in data cleaning, and the impact of peer review.

We find meaningful differences in the ways that different researchers approach the same task. Some of these differences come from decisions that would normally receive scrutiny from a reader, like the research design and choice of control variables. Other differences came from sources where researchers choose differently but a reader might not recognize that a consequential decision had been made, such as in the functional form of the control variables or in a number of data cleaning or sample limitation decisions. When we force researchers to use the same research design, results became more similar, especially when that shared design is rigidly adhered to. Researcher agreement increased sharply when pre-cleaned data was provided to researchers, implying that data cleaning decisions are a major source of variation between researchers. Development of more mature and standardized data cleaning procedures, and increased visibility for data cleaning and the sharing of data cleaning code, may have a meaningful impact on the consistency and believability of results in applied microeconomics.

## Previous Work on Research Reliability and Researcher Degrees of Freedom

In economics, suspicion about empirical results is not new [@leamer1983let]. The most recent wave of concern is inspired by discussions, originating in the field of psychology, of the "replication crisis." Studies on the replication crisis show that a high percentage of studies cannot be replicated when tested using new data [@open2015estimating;@camerer2016evaluating], that study code and data are not available or do not reproduce the published results [@herbert2021reproducibility], or that "policing replications" that test sensitivity of published results are rare [@ankel2023economists].

While this prior replication work takes an existing study as a baseline and asks whether it is robust to re-evaluation in some way, questions about researcher degrees of freedom are not about whether a given study can be challenged, but instead whether a different researcher performing the same study would have done it differently. These two fields may intersect, and some failures to replicate in replication studies could be due to researcher degrees of freedom, where both the original study and the replication made reasonable choices but found different results [@bryan2019replicator]. The difference in framing here is that the replication literature views the differing choices as a challenge to the validity of the original results, while the researcher degrees of freedom framing views both as part of a universe of reasonable results, assuming both analyses are defensible.

One way to empirically study researcher degrees of freedom is using a many-analysts design. The many-analysts design, popularized by @silberzahn2018many, gives the same data set to multiple teams of researchers and has them independently try to answer the same research question.[^2] Many-analysts studies have now been carried out in many fields, including microeconomics [@huntington2021influence], finance [@menkveld2021non], religion [@hoogeveen2023many], neuroimaging [@botvinik2020variability], political science [@breznau2021observing], machine learning [@chen2024subjectivity], ecology and evolutionary biology [@gould2023same], psychology [@boehm2018estimating; @bastiaansen2020time; @schweinsberg2021same], and medical informatics [@ostropolets2023reproducible], among others.

[^2]: Many-analysts designs are sometimes referred to as "crowdsourced" science.

With few exceptions, many-analysts studies find that there *is* meaningful variation in both methods and conclusions across researchers. Furthermore, researcher variation in design and analysis likely outweighs population variation in effects [@holzmeister2023heterogeneity].

However, these studies vary considerably in the extent to which they can identify the source of that researcher variation or suggest policies that might reduce it. Establishing that there is variation is important, but is of limited impact if we do not understand why or what we can do about it. Further, if not carefully performed, many-analysts results may not even imply that a problem exists. For example, variation in the original @silberzahn2018many study may be largely explained by the research question not being made sufficiently clear to researchers [@auspurg2021has], and not following standard meta-analytic practice may lead us to overstate variation between researchers by being too sensitive to outlier estimates [@auspurg2023social].

The ability to explain variation between researchers, rather than just show that variation exists, is restricted by the limited size of the prior many-analyst studies. As we discuss in @sec-target-sample, we pursued a sample of at least 90 researchers to achieve sufficient power to explain differences in variation.[^replication-sample] Since participation in a many-analysts study takes considerable time and effort, sample sizes are often well below even the aforementioned 90, which may explain why many studies do not attempt do decompose the variation in effects they find between sources. These smaller sample sizes can produce acceptable statistical power for some tests but not others, and explaining variation or agreement in effects between researchers generally demands a larger sample than showing the existence of meaningful variation or showing a difference in rates of making a particular research decision. Prior many-analyst studies that try to explain the sources of variation between researchers either do so despite the low-power issue, gather larger samples of researchers, or select analyses that produce adequate power despite small samples.

[^replication-sample]: @perignon2022reproducibility, in looking at the sources of reproducibility variation using many teams, used a design with 1,000 tests to replicate in order to adequately power comparisons. 

Among studies that attempt to explain researcher variation, there are three common explanations explored: difficulty of the research task, differences in researcher experience or characteristics, and the presence of peer review or evaluation. Some studies show less researcher agreement in more complex or difficult-to-analyze scenarios [@menkveld2021non; @ortloff2023different]. Higher-quality teams (with more experience, seniority, publishing success, and/or people) agree more [@menkveld2021non], experienced researchers tend to draw more abstract codebooks and conclusions than students [@ortloff2023different], and  replicators with more coding skill found more errors in original work [@broderick2020automatic]. Outside of many-analysts work, @jelveh2024political find that researcher political orientation affects research results, and @sulik2023scientists show the same for researcher personality metrics. However, other many-analysts research finds that researcher characteristics explained only a small share of the variation in results [@breznau2021observing]. Finally, review may increase agreement [@menkveld2021non]. However, in some cases there is no chance to revise, so we cannot see the impact of peer review, but instead outside evaluation is used as a measure of researcher quality, although in that case, peer review scores do not predict whether a given researcher produces an outlier result [@gould2023same].

Outside of many-analyst designs, there are studies that use simulation to try many combinations of analytical or data-cleaning choices and examine the resulting variation in estimates. This approach is similar to a many-analysts design in that they look at variability in potential research choices and, often, try to explain variation in effects estimates using those choices. These studies are necessarily limited to the set of research decisions that the project organizers consider ahead of time (which constrains the universe of possible decisions but also makes interpretation of the results far more clear), and typically consider all combinations of decisions equally, rather than favoring combinations an actual researcher would choose. Of these studies, the closest to the present study evaluates the sensitivity of results in an observational psychological data set to different data preprocessing and modeling choices [@klau2023comparing]. They try multiple combinations of reasonable preprocessing and modeling choices and use simulation to iterate through the universe of potential choices, finding significant variation in effects over these choices. A similar attempt to separate researcher variation into modeling and preprocessing components is also done in a many-analysts design in @huntington2021influence, although in a limited way.

This study's design aims to evaluate multiple of these variation sources through a staged design, similar to @perignon2022reproducibility. The different stages allow different degrees of researcher choice along the lines of interpretation of the research question, research design, and data preparation, as well as randomized peer review. The goal is to incorporate the mechanisms proposed by the literature and responding to the critique of @auspurg2021has. Researcher characteristics are collected, as well, allowing for exploration of the researcher-characteristics source of researcher variation, although not in a controlled way. We do not address the difficulty of the research task as a potential source of researcher variation in this study.

# Design

In this study, we aim to isolate the influence of several different potential sources of researcher variation by having the same set of researchers complete the same research task at least three times. We refer to these main research tasks as Task 1, Task 2, and Task 3. Following each task, a subset of researchers are randomized into a round of peer reviews, and given the opportunity to revise their work.

Task 1 gives researchers a large amount of freedom in how they complete the research task. Each successive task removes a degree of freedom from the researcher and further specifies how the analysis is to be performed. The intuition behind this design is that if the removal of a specific kind of researcher freedom meaningfully reduces the variation in results between researchees, then that degree of freedom is a meaningful contributor to researcher variation.

The following goals and instructions are shared across all tasks:

-   Estimate the causal effect of a policy on a specified outcome, among the group affected by that policy (see @sec-focaltask below for more details).

-   Use American Community Survey (ACS) data to estimate the effect, using data no older than 2006 and no newer than 2016.

-   Procure ACS data from IPUMS [@ruggles2024ipums], selecting only one-year files and using harmonized variables.

-   Optionally, combine the ACS data with a data set on the presence or absence of other relevant policies, provided by the organizers.

-   Use a statistics package or language that allows results to be immediately replicated.

Researchers were also given background information on the policy itself and its eligibility criteria, guidance on how to use the IPUMS website, instructed to use assistants for any work they would normally use assistants for, and to complete their analysis as though it had been their own idea, rather than attempting to match or not-match other researchers, or asking the project organizers how they would like the analysis to be performed.

These instructions comprise the entirety of the limitations on researchers in Task 1. Tasks 2 and 3 specified the task further and removed researcher degrees of freedom.

Task 2 specified the research design more precisely. Instead of allowing any research design to identify the causal effect of interest, Task 2 gave specific definitions for which individuals comprised a "treated" group and which comprised an "untreated" comparison group.[^3] Then, it instructed researchers to estimate the effect by comparing how outcomes for the "treated" group changed from before policy implementation to afterwards against how outcome for the "untreated" group changed. This can be thought of as a difference-in-differences style design, although the phrase "difference-in-differences" was not used in the instructions.

Task 3 uses the same research design limitations of Task 2, but also provides a pre-cleaned data set, prepared by the organizers. The data set offered a pre-prepared treated/untreated-group indicator as specified in Task 2, limited the data set only to the treated and untreated group, prepared and cleaned all variables in the data set that did not already come pre-cleaned, handled missing-data flags, merged in state policy data, and offered standardized simplified recodings of demographic variables. Researchers were instructed to not further clean the data or limit the sample.

[^3]: Although eligibility criteria for the policy were explicitly given in Task 1, Task 2 further limits the treated group by narrowing the acceptable age range. The limitation was more impactful for defining the untreated comparison group, though. Many researchers did use a treated/untreated group approach in Task 1 before it was specified in Task 2, but researchers defined the untreated group in highly diverse ways, as will be shown in the Results section.

Comparison of the researcher output between Task 1 and Task 2 is intended to show the researcher variation introduced by either an imprecise statement of the research question, as in @auspurg2021has, or due to differences in research design choices.

Comparison of the researcher output between Task 2 and Task 3 is intended to show the researcher variation introduced by decisions made in the data cleaning and variable definition process. A researcher following the Task 2 instructions should arrive at the same sample size, number of treated individuals, and number of untreated individuals as in Task 3, as well as the same definition for the outcome variable.[^4] Differences in the data set and in the results between Task 2 and Task 3 should be a result of differences in the data cleaning and preparation process.

[^4]: The Task 2 instructions do leave some leeway for definition of some variables, in particular control variables like education or race, which have a specific recoded version available in Task 3 that are not specified in the Task 2 instructions. However, the definitions of the treated and untreated comparison groups should be the same between Task 2 and Task 3.

Following each of the research tasks, researchers engage in a round of peer review with 2/3 of researchers randomly assigned to peer review and 1/3 who not assigned to peer review. Those in peer review are randomly assigned in pairs. Those pairs performed a blind review of each others' work, and provided a written assessment of that work. Reviewers were instructed to produce a review "as though (they) were the reviewer of a journal article," and to judge the work as though they were reviewing for a journal where a study of this kind "could be published if the work was of high quality."

Following peer review, researchers have an opportunity to revise their work in light of the peer review (or for any other reason). Importantly, revision is not mandatory, nor is satisfying one's peer reviewer, and the majority of researchers choose not to submit revisions.

Notably, this form of peer review does not exactly match what is typically done in peer review work for journal publications. In particular, revision is not mandatory, all reviewers have themselves completed a study with the same goal and data and so have extensive background information, and all reviewers are themselves also reviewed by the same person. These features will all affect interpretation of the peer review results. In particular, the non-mandatory nature of the peer review means that the between-round revision work is only visible for a small subset of the researchers, and the paired nature of the reviews means we cannot separate the effect of being reviewed from the effect of reviewing someone else.

Following each research task and revision, researchers filled out a survey about their work.[^5] This survey asked them to report their findings, additional information like sample size and standard errors, and choices made in the process of doing the analysis like sample restrictions, treated-group definitions, estimator, and standard error adjustments. Researchers were also asked to justify why they had made these choices.

[^5]: Note that the design of this study, and this survey, predates @sarafoglou2024subjective and so does not follow it.

This research design and analysis plan has been preregistered [@portner_huntington-klein_2022]. Analyses that were not preregistered will be noted in the results section as they are performed. Full instructions for each task, as well as post-task survey text and the peer-reviewing instructions, are available in the online appendix.

# Data

## The Focal Research Task {#sec-focaltask}

In all research tasks, the specific goal given to researchers was:[^6]

[^6]: Full instructions are available in the online appendix.

> Among ethnically Hispanic-Mexican Mexican-born people living in the United States, what was the causal impact of eligibility for the Deferred Action for Childhood Arrivals (DACA) program (treatment) on the probability that the eligible person is employed full-time (outcome), defined as usually working 35 hours per week or more?
>
> DACA was implemented in 2012. Examine the effects on full-time employment in the years 2013-2016.

In simple terms, this asks researchers to estimate the impact of the DACA program on the probability that those eligible for the program usually work 35 hours per week or more in the years 2013-2016.[^7]

[^7]: There are several existing papers that use the same ACS data set to identify the effect of DACA on various outcomes. The design used in Tasks 2 and 3 was most directly inspired by @amuedo2016can, although the designs do not match exactly, and the outcomes of interest are not the same. Researchers are informed that such previous studies exist and that they can optionally look into previous studies for background as they would normally do when performing research, although no specific previous study is listed. The instructions emphasize that any previous study does not constitute a "right answer" that researchers should be trying to match.

Researchers, many of whom are not from the United States and so may not be familiar with DACA, are given further background information about the DACA program:

-   DACA allowed undocumented immigrants who were accepted into the program to have legal work authorization for two years without fear of deportation, and also allowed them to apply for drivers' licenses or other forms of identification. People could reapply after the two years expired, and many did.

-   Applications for the program opened on August 15, 2012, and over the first four years of the program's existence, over 900,000 applications were received, about 90% of which were approved.[@citservices2016]

-   While the program was not specific to immigrants from any origin country, because of the structure of undocumented immigration to the United States, the great majority of eligible people were from Mexico.

Researchers were also given information on the eligibility criteria for DACA, which was intended to apply only to a specific subset of undocumented immigants who arrived in the United States as children, and not to all undocumented immigrants. Eligible people must:

-   Have arrived in the United States before their 16th birthday.

-   Not have had their 31st birthday as of June 15, 2012.

-   Have lived continuously in the United States since June 15, 2007.

-   Were present in the United States on June 15, 2012 and did not yet have legal status (either citizenship or legal residency) during that time.

An additional eligibility requirement was mistakenly omitted from the Task 1 instructions, but was included for Tasks 2 and 3:

-   Eligible people must have completed at least high school (12th grade) or be a veteran of the military.

In addition to this information about the policy itself and the effect that researchers are supposed to identify, researchers were also given instructions about the data set to use and how to procure it, as well as some details on usage of the data:

-   Data should come from the American Community Survey (ACS), using data no older than 2006, and no newer than 2016.

-   In addition, a file of state/year-level data was provided including labor market data and the presence or absence of different immigration policies in different years. Immigration policy data comes from @urbaninstdata.[^8]

    ACS data should be procured from the IPUMS website [@ruggles2024ipums], specifically selecting one-year ACS files and harmonized variables. Written and video instructions were included showing how to select data samples and variables on the IPUMS website.

-   Researchers were not told which specific variables to use to determine eligibility status, but they were given guidance onto how to find relevant variables (like looking at the Person $\rightarrow$ Race, Ethnicity, and Nativity page to find variables relevant to ethnicity, birthplace, citizenship, and year of immigration).

-   Several relevant features of the ACS that may affect analysis were emphasized: (a) ACS is a repeated cross-section, not a year-to-year panel data set, and (b) ACS does not list the month that data was collected in, so it is not possible to distinguish whether a given observation in 2012 is from before or after the policy was implemented, and (c) we do not actually observe in ACS whether a given person is enrolled in DACA, so we assume that all eligible people who are ethnically Mexican and Mexican-born are treated.

[^8]: This file included the state/year-level unemployment rate and labor force participation rate. Immigration policy flags were for policies for undocumented immigrants to get state drivers' licenses, to get college financial aid, to be banned from state public colleges, or to follow Omnibus immigation legislation that serves to increase the surveillance of immigation documentation. Additional indicators were for participation in E-Verify laws that require employers to verify immigration authorization, to limit E-Verify participation, participation in Secure Communities, and for participation in task-force or jail based 287(g) policies.

Finally, researchers were instructed to keep track of any variables used to limit their sample download on IPUMS, and to review the survey where they would be reporting their results before beginning their analysis.

From there, researchers were given free reign to complete the analysis as they thought most appropriate, including their own choice of statistical software, an instruction to use assistants for any work that they might normally use assistants for, and asking them to complete the analysis as they thought best, as though the research task had been their own idea, not trying to match or not-match other researchers or guess what analyses the project organizers wanted to see. Once finished, they uploaded all of their code and data to a Sharepoint website, wrote a short description and interpretation of their results focusing on a single "headline" result, and filled out the research survey to report their results.

For Task 2, all of the previous instructions remained in place, but several were added to further specify the research design:

-   There is a "treated" group that is comprised of all ethnically Mexican and Mexican-born individuals who are aged 26-30 on June 15, 2012 (recall that individuals must not have had their 31st birthday as of June 15, 2012 to be eligible for DACA).

-   There is an "untreated" group that is comprised of people who would have been eligible for DACA, except that they were aged 31-35 on June 15, 2012.

-   Researchers should estimate the effect of treatment by seeing how the 26-30 group changed from before treatment to after relative to how the 31-35 group changed (keeping in mind this is a repeated cross-section and not panel data).

-   Researchers should attempt to estimate the effect for all individuals in the "treated" group and not, for example, estimate the effect only for men or only for women.

-   The instructions specifically mention that researchers can, if they like, use covariates or account for differing trends to improve the comparability of the treated and untreated groups.

The task is otherwise unchanged for Task 2.

In Task 3, the instructions remain unchanged from Task 2, except that the data is provided directly instead of having researchers download data from IPUMS, omitting data from the year of 2012. In Task 3, project organizers cleaned the data, merged in the state policy data, created a variable indiciating whether a given individual was in the "treated" or "untreated" group, limited the sample only to individuals in "treated" or "untreated," and created simplified versions of variables like education. Researchers were instructed not to further limit the sample from this prepared data set, or to perform further extensive data cleaning.[^9]

[^9]: There were three observations in the final cleaned data set that were missing values of the education variable. The final used sample in Task 3 sometimes differs by 3 across researchers, based on whether the analysis uses education and thus drops these individuals.

## Recruitment and Attrition

In a many-analysts study, researchers who carry out the research task make up both the bulk of the author list and are the subject of inquiry, so their recruitment is a key feature of the study.

### Researcher Qualifications

The goal of the project organizers was to make the set of researchers representative of the set of people who produce the applied microeconomics literature. As such, recruitment criteria focused on identifying people who have produced applied microeconomic research, including potentially non-academic applied microeconomics research.

A given researcher was qualified for the project if they satisfied any one of the following criteria:

-   They are academic faculty working in applied microeconomics.

-   They are a graduate student **and** have a published or forthcoming paper in applied microeconomics.

-   They hold a PhD **and** work in a job where they write non-academic reports using tools from applied microeconomics to estimate causal effects.[^10]

[^10]: This qualification would allow, for example, employees of the World Bank, or people working in private sector research, to participate.

Participation was not limited on the basis of country, career stage, or demographics such as sex, race, or sexual or gender identity.

### Target Sample Size {#sec-target-sample}

An initial simulation-based power analysis assumed that each research task would have 5% less between-researcher variation in observed effects than the previous round and looked at the statisical power to detect a linear relationship between round number and the squared deviation of effects (variance of estimated effects across researchers). We found that we had 90% power to detect this effect if 90 researchers finished all tasks. We also found that, for comparisons of only two different research tasks, 90 researchers would give 85% power to detect a decline in variance from one stage to the next of 15% or more, a reasonable effect size given previous many-analyst studies.

We further assumed that attrition rates would be roughly 50%, which would suggest recruiting 180 eligible researchers to achieve adequate power. We revised that goal to 200 to account for our assumptions potentially being optimistic. Project organizers obtained funding to support payments to 200 researchers (see below).

### Recruitment and Incentives

Recruitment was advertised to potential researchers through three avenues: (1) social media posts on Twitter and LinkedIn, (2) emails to professional organizations including the Institute for Replication and the Committee on the Status of Women in the Economics Profession, and (3) emails to United States economics department chairs. For emails to departments heads, we gathered the list of all 286 economics departments listed in the U.S. News and World Report. We could locate email addresses for a front desk or (preferably) department chair for 264 of those departments. We emailed those 264 departments, asking for the message to be passed on to all faculty or just all microeconomics faculty.

The recruitment message described the project and its goals, and provided a link to a website that included further detail on project expectations and incentives for participation.[^11] Researchers were told that if they completed all three tasks of the project, they would be offered a \$2,000 payment for up to 200 of the participants, and authorship on the eventual paper. The website included a link to a survey that asked questions related to eligibility for the project.

[^11]: <https://nickch-k.github.io/ManyEconomists/>

### Participation and Attrition

```{r}
source('../code/participation_and_attrition.R')
```

Overall participation and attrition values are in Table \ref{tbl-attrition}. `r justcount[1,Participants]` people submitted applications for the project with `r justcount[1, Attrition]` of these found to be ineligible for the project. Most ineligible people were graduate students who did not yet have a forthcoming paper.[^12]

[^12]: Data processing and analysis as well as table and figure creation for this paper were performed using the R packages **data.table**, **tidyverse**, **rio**, **fixest**, **car**, **modelsummary**, and **vtable** [@R-data-table; @tidyverse2019; @R-rio; @fixest2018; @car2019; @modelsummary2022; @R-vtable].

```{r attrition}
justcount |> knitr::kable(booktabs = TRUE,
                          caption = 'Participation and Attrition\\label{tbl-attrition}')
```

This left `r justcount[2,Participants]` eligible participants. This is more than the 200 for which budget was available to pay the offered \$2,000 incentive. The 282 of these participants who had signed up by the original signup due date were put into a random order, and then the 13 late signups were put at the end of this order. Participants were given their place in the order, and informed that, among people completing all stages of the project, the first 200 in the order would be paid.

Initial assumptions from the power analysis that attrition rates would be near 50% were almost exactly correct, with `r percent(justcount[5,Participants]/justcount[2,Participants],.01)` of these initial `r justcount[2,Participants]` eligible researchers completing all three stages. Nearly all of the attrition occurred by the completion of Task 1. After `r justcount[2,Participants]-justcount[3,Participants]` eligible researchers failed to complete Task 1, only a further `r justcount[3,Participants]-justcount[5,Participants]` failed to complete Task 3. This means we have `r justcount[5,Participants]` researchers who completed all three research tasks, well above the goal of 90.

The high recruitment numbers and the fact that nearly all attrition occurs before Task 1 is complete allows us to evaluate the impact of the payment incentive. One potential concern with our incentive design is that payment and authorship are offered to anyone who completes all tasks, regardless of the quality of their work. We evaluate whether being guaranteed payment affects the probability of completing Task 1 using a regression discontinuity design. Someone randomly assigned to position 199 in the ordering is guaranteed payment if they complete all the tasks, while someone in position 201 may think they are likely to receive payment, but they are not guaranteed it.

```{r}
#| label: fig-rdd
#| fig-cap: Impact of Guaranteed Payment on Probability of Task 1 Completion

rdplot(moneyatt$Finished,moneyatt$order,200,
       x.label = 'Order',y.label = 'Prob. Completed First Task',
       title = '')
```

@fig-rdd shows no meaningful effect of being guaranteed payment on the probability of completing Task 1. In additional results in the appendix, using a linear regression specification of the regression discontinuity design and the full range of the data (not including the late sign-ups) to maximize statistical power,[^13] we again find no statistically significant effect of being guaranteed treatment. This is suggestive that participants were not simply signing up in an attempt to get a \$2,000 payment for little effort.

[^13]: Use of the full range, rather than a bandwidth, is justified given that the running variable is randomly assigned aside from the late sign-ups. We also find no effect if we drop the late sign-ups from the regression discontinuity analysis.

### Sample Characteristics {#sec-sample-characteristics}

Tables \ref{tab-samp1} to \ref{tab-samp3} show the characteristics of the recruited sample, and how those characteristics changed with eligibility and attrition. Task 2 is omitted as an attrition stage since so few people dropped out between Task 1 and Task 2.

Table \ref{tab-samp1} shows that the majority of researchers were recruited via social media, with only about 9% coming from a department email, 4% from a professional organization email, and 9% from some other source (like word-of-mouth). Those recruited from another source were less likely to qualify for the study, and slightly less likely to finish, while those recruited from social media were most likely to qualify and finish. We also asked researchers how certain they were of their ability to finish the first task as well as the full set of tasks, on a scale of 1 to 100. Enrollees were about 90% confident in their ability to complete the full set of research tasks (although only about 50% did). Those who were more confident were slightly more likely to actually finish, and average confidence rates of those who did finish were about 92% instead of 90%.

```{r}
#| output: asis
sumtable(alldemog, vars = c('Researcher_Q11',
                            'Researcher_Q12_1',
                            'Researcher_Q12_2'), 
         labels = c('Recruitment Source','Certainty to Finish Task 1','Certainty to Finish Task 3'), group = 'Round',
         out = 'latex',
         title = 'Researcher Recruitment Source and Completion Confidence',
         fit.page = '\\textwidth',
         anchor = 'tab-samp1')
```

Table \ref{tab-samp1} shows the professional experience of enrollees. While graduate students were considered eligible for the project as long as they had a published or forthcoming paper, the majority of eligible researchers (`r alldemog[Round == 'Assigned task 1',percent(mean(Researcher_Q10 %in% c('PhD','Prof. Degree')),1)]`) had PhDs. PhD holders were also more likely than other eligible researchers to complete all three tasks.

These PhDs are split across faculty (`r alldemog[Round == 'Assigned task 1',percent(mean(Researcher_Q6 %in% c('Faculty')),1)]`) and other non-faculty researchers (`r alldemog[Round == 'Assigned task 1',percent(mean(Researcher_Q6 %in% c('Other Researcher')),1)]`), both of which were more likely than graduate students to finish all three rounds. Note that the researchers in these categories who do not hold PhDs were either people who had been hired to faculty roles without holding PhDs (such as ABDs, or people in a faculty position requiring only a Master's degree), or people with Master's degrees in non-faculty research positions who had published academic papers (some of whom were still graduate students).

Most of the researchers had at least one published paper, and researchers with 6+ papers were more likely than others to complete all three research tasks. Those with "No Academic Papers" are non-academic researchers who produce work not intended for academic journal publication. Those with "No Published Academic Papers" have papers that are forthcoming, or are faculty who only have working papers and no publications.

The set of researchers in the study generally do not work in the specific subfield that the research task is in. The research task is similar to many studies done across all of applied microeconomics, but specifically is on the topics of labor and immigration. About a third of the enrollees had done research in either immigration or labor previously, and these researchers were somewhat more likely to complete all three tasks. No researchers enrolled who had previously worked in both immigration and labor.

```{r}
#| output: asis
sumtable(alldemog, vars = c('Researcher_Q10','Researcher_Q6', 'Q8Recode','Researcher_Cats'), 
         labels = c('Degree','Occupation', 'Research Experience','Field'), group = 'Round',
         out = 'latex', fit.page = '\\textwidth',
         title = 'Researcher Professional Experience',
         anchor = 'tab-samp2')
```

While the research tools used are not listed in Table \ref{tab-samp2}, we did check the programming languages used by researchers. The most common language was Stata, with 109 researchers performing their work solely in Stata. 33 used R, and one researcher used both R and Stata. Less common were Python and SPSS, with one researcher each.

Table \ref{tab-samp3} shows the demographics of the researcher sample. The eligible sample was just under 80% male and more than 55% white, and both percentages grew by the conclusion of Task 3, with the white share growing significantly to 66%. The 80% male figure is similar to the share male found for faculty at a selected set of top economics departments in 2017 by @lundberg2019women, and among all actively publishing economists in 2019 by @card2022gender. A small share reported being LGBTQ+, and this share remained constant over all rounds of the research tasks. An additional form of demographic difference is geographic. About half of the sample was situated in the United States, and about half was from another country.[^14] The representativeness of the racial mixture is difficult to assess for this reason; 66% white would be low if the entire sample were from the United States [@stansbury2023economics], but it is unclear what the population rate is in a 50% US/50% other location sample.

[^14]: Exact figures are not given for geography, and crosstabulations across geography are not given, because non-geographic demographic information comes from a survey where we acquired permission to share aggregate figures. Geographic information, on the other hand, comes from researcher payments information, for which we did not request permission to share responses.

```{r}
#| output: asis
sumtable(alldemog, vars = c('Researcher_Q15',
                            'RaceRecode',
                            'Researcher_Q17'), 
         labels = c('Gender','Race','LGBTQ+'), group = 'Round',
         out = 'latex',
         title = 'Researcher Demographics',
         fit.page = '\\textwidth',
         anchor = 'tab-samp3')
```

```{r}
# OMIT THIS TABLE, BUT FROM IT WE GET 1 completed Python, 1 SPSS, 1 R/Stata, 33 R, 109 Stata in completed
# alldemog[Q2 == 'Finished task 3'] |>
#   sumtable(vars =  c('Researcher_Q10','Researcher_Q6', 'Q8Recode','Researcher_Q15',
#                             'RaceRecode',
#                             'Researcher_Q17',
#                      'Researcher_Q11',
#                      'Researcher_Cats',
#                      'Language'), 
#          labels = c('Degree','Occupation', 'Research Experience','Gender','Race','LGBTQ+','Recruitment Source','Field',
#                     'Coding Language'),
#          col.breaks = 4,
#          out = 'latex',
#          fit.page = '\\textwidth',
#          anchor = 'tab-samp4')

```

One researcher did complete all three research tasks, and appears in the above tables, but their work has been removed from the results that follow in the rest of the paper, because a misunderstanding of the instructions meant that their work did not attempt to estimate the effect of DACA on the probability of employment.

```{r}
# Ensure the proper analytic sample for all following code
dat = dat[Q1 != 972]
dat = dat[Q1 %in% Q1[Q2 == "The third replication task"]]
```

As a whole, the sample largely reflects the group of people who publish work in applied microeconomics. The sample is skewed towards the United States, which is partially driven by the emails sent to US economics departments, the fact that the project was advertised and carried out in English, and the fact that the project organizers are in the United States and advertised the project using their own social media. Given that caveat, the makeup of the sample appears to be fairly similar to the makeup of the profession itself, although this is difficult to verify for some demographics.

# Results

This section examines the variation in effects and methods across researchers and conditions, demonstrating that variation exists and attempting to explain it.

Importantly, these results are derived from the survey responses that researchers gave about their findings and the choices made. Project organizers did not cross-reference survey responses against researcher code to ensure that their code was accurately reflected in the survey, except in a small number of cases where the survey response could not be interpreted. This means that the variation presented here represents the variation in how researchers would plan to implement the research task if they were doing it independently, and what a reader would see as the description of a study in a published version of their work. Any variation between researchers that occurs as a result of coding error or a research report that misrepresents what a researcher actually did will not be reflected here, but could be the subject of a future investigation.

## Variation in Effects and Sample Sizes {#sec-variation}

```{r}
source('../code/variation_in_effects_and_sample_sizes.R')
source('../code/variance_tests.R')
```

@fig-effect-distributions and Table \ref{tab-effectdist} show the distribution of estimated effects across all researchers. The effect distributions are shown in two ways: unweighted and using inverse-standard-error weights.[^15] Several data points are dropped from the weighted analysis for researchers who did not report standard errors or reported 0. Other missing values are researchers who did not repond to a given question.

[^15]: The use of inverse-standard-error weights is not preregistered but follows meta-analytic standards, reducing the influence of estimates that may be outliers due to being estimated with a highly-noisy method, under the suggestion of @auspurg2023social. Weights are truncated at the 95th percentile (200, or a standard error of .005) so as to avoid any single researcher having too much influence on results. Not using the truncation leads to more agreement because a few researchers with very small standard errors make up a significant share of the weighted sample.

In Task 1, the mean estimated effect of DACA eligibility on the probability of working full-time was .053 unweighted or .044 weighted. In both cases these means are pulled upwards by high top-end estimates and are above the 75th percentiles. Median estimates were .030 unweighted or .026 weighted. Even in Task 1, with a large amount of freedom afforded, researchers found a reasonable amount of agreement in the effect sizes outside of the tails, with the 25th to 75th percentile ranges of the effect being .014 to .051 unweighted, an inter-quartile range (IQR) of .037, or 3.7 percentage points in the effect, or .012 to .043 weighted, an IQR of .031. The use of weights narrows the distribution of effects: researchers reporting smaller standard errors also reported estimates that were more similar to each other, which was also the case in Tasks 2 and 3.

Our preregistration plan details that we planned to give descriptive characteristics of the results, as we do here, and also implement a Levene test on whether the variance between researchers declined. We do not reject at the 95% level the null of no change in variance from any stage to any later stage (including a comparison of each task to its revision stage, and comparing each main task to later main tasks), with the lowest p-value of `r number(levene_test_results$pval[1],.001)` coming from the comparison of `r levene_test_results$TaskA[1]` to `r levene_test_results$TaskB[1]`.

Task 2 is somewhat odd in that it shows less agreement than Task 1 despite giving researchers less freedom. The IQRs increase to .043 unweighted or .040 weighted. Further, the effect distributions are somewhat bimodal, especially when weighted. One of these modes appears to be researchers reporting effect estimates of a similar level to those in Task 1, and others reporting effect estimates similar to what would later be found in Task 3.

Moving all the way to Task 3, agreement considerably increases between researchers. The 25th and 75th percentile effects are .031 and .058 unweighted (IQR .027), and .036 and .060 weighted (IQR .024). The bimodality from Task 2 is still there, but with much more agreement and density at the higher mode. From Round 1 to Round 3 we see considerable increases in agreement between researchers.

Taking only the effect distributions as a baseline, we see that, at least in this application, researchers in general report fairly similar, although certainly not identical, effect estimates on average, but there are some extreme outlier estimates as well. We may also take this to mean that providing pre-cleaned data, as in Task 3, led to a strong increase in researcher agreement. Specifying further the research question and design, however, as in Task 2, led to somewhat less agreement. The odd result for Task 2 and its proper interpretation will be investigated further in @sec-bimodal.

```{r}
#| fig-width: 8
#| fig-height: 5
#| label: fig-effect-distributions
#| fig-cap: Distributions of Reported Effect Sizes
p_effect_distribution
```

Table \ref{tab-effectdist} also shows the reported standard errors. Reported standard errors increase significantly from round to round, driven largely by the specification of the research design, which for many researchers considerably narrowed the sample they were supposed to use. This can also be seen in @fig-full-effect-distribution, where the distribution of effects narrows a little across rounds, but confidence intervals increase considerably. Throughout, while there is general agreement on effect size in the middle of the distribution, researchers vary in whether the reported effect is statistically significant, with `r viol[Round == 'Task 1' & Type == 'Weighted', percent(mean(sig, na.rm = TRUE))]`, `r viol[Round == 'Task 2' & Type == 'Weighted', percent(mean(sig, na.rm = TRUE))]`, and `r viol[Round == 'Task 3' & Type == 'Weighted', percent(mean(sig, na.rm = TRUE))]` reporting results that were statistically significantly different from 0 in Tasks 1, 2, and 3, respectively.

We can compare average standard errors against the variation in effects between researchers to get a sense of how much effect variability is omitted by only considering a reported standard error, as in @huntington2021influence or @menkveld2021non. Comparing the standard deviation of weighted effects against the average standard error gives ratios of 4.84, 2.23, and 1.75 for Tasks 1, 2, and 3, respectively. @huntington2021influence used a single round and allowed full researcher freedom, and found a range of 3-4 for this ratio, below what we find for the full-freedom Task 1. If we instead compare weighted IQR to average standard errors we get ratios of 1.63, 1.29, and .41. This is partially driven by increasing agreement over rounds, but is also driven by the addition of restrictions that reduce sample size and thus increase average standard errors. These figures suggest that a reported standard error considerably understates the estimate uncertainty that we should acknowledge when including researcher variation. However, these figures also demonstrate a flaw with these ratios, introduced in @huntington2021influence, as a metric for researcher-indused uncertainty: in that they are sensitive to the estimate precision one might be expected to get given the research task. Researcher variation does not scale with that uncertainty.

```{r}
#| output: asis

dftoLaTeX(effect_sum_tab,
         anchor = 'tab-effectdist',
         title = 'Distribution of Reported Effects and Sample Sizes', fit.page = '\\textwidth') |>
  cat()
```

```{r}
#| label: fig-full-effect-distribution
#| fig-cap: Specification Curve for All Reported Estimates
p_full_effect_distribution_individual
```

Table \ref{tab-effectdist} also shows summary statistics for reported standard errors, both overall and for the treated group. These distributions are also shown in @fig-sample-size-distributions and @fig-treated-group-distributions.

In @fig-sample-size-distributions we see a huge amount of variation in the reported sample size used in Task 1, noting that the x-axis is on a log scale. The 25th and 75th percentiles of reported sample sizes ranging from 61,600 to 356,787, with some researchers using millions of observations.[^16] For Task 2, which specified in the instructions the treated and comparison groups to use, variation reduces considerably, although the 75th percentile (48,125) is still double the 25th (18,981), and there are still some researchers using millions of observations. Task 3 is not shown in the graph because the sample is pre-specified, with the only meaningful variation being whether or not the researcher dropped three rows of data with missing education values, and a few outliers reporting lower numbers. The lower sample sizes for this question are due to researchers who skipped it because they assumed the answer was obvious.

[^16]: Keep in mind that for Task 1, there was not a specified control group, so a researcher may decide to use the entire ACS sample in the analysis, including people very unlike the eligible group in the sample. In Task 2, the instructions specified a treated and comparison group, but some researchers may have different samples than in Task 3 either due to error, or because they included people other than the treated and comparison group in their sample to improve precision, and used their model to compare those groups more directly.

```{r}
#| label: fig-sample-size-distributions
#| fig-cap: Distributions of Reported Sample Sizes
#| fig-width: 8
#| fig-height: 5
p_sample_size_distributions
```

Variation in the reported size of the treated group in @fig-treated-group-distributions is affected somewhat by researcher confusion in responding to the survey question. The survey question about treated-group size instructed researchers not to count individuals eligible for DACA as treated for the purposes of this question if they were in a pre-DACA year. However, many researchers counted these individuals as treated anyway, leading to variation in the Task 3 distribution, even though every researcher is at this point working with the same eligibility indicator.

```{r}
#| label: fig-treated-group-distributions
#| fig-cap: Distributions of Reported Treated-Group Sizes
p_treated_group_sample_size
```

Aside from this issue, we see that the imposition of a shared definition for the trated group reduced the IQR for the size of the group from 34,631 in Taks 1 to 9,879 in Task 2. Theoretically, however, since there was a shared definition of the treated group in Task 2, there should be no more variation in this variable in Task 2 than in Task 3. This indicates that not all instructions were implemented in the same way across researchers, which will be explored further in @sec-sample-limitations. Despite a shared understanding of who was eligible for DACA and who should be in the treated group, only a shared data preparation that implemented these rules for people led to sharp agreement in the size of the treated-groups sample.

```{r}
# this section used to be here,
source('../code/researcher_characteristics_and_effects.R')
source('../code/researcher_characteristics_and_effects_b.R')
```

## Peer Review

This section evaluates the impact of peer review on the later work performed by a researcher. The structure of peer review in this study is that, following each main task, 2/3 of the researchers are randomized into pairs that produce a peer review report of the other's work, while the remaining 1/3 do not receive or perform peer review. Then, researchers have an opportunity to revise their work. 

Revision is optional, and relatively few researchers (less than 30 per task) chose to revise their work after receiving peer review. As such, we mostly look at the impact of peer review on the work performed in subsequent main tasks. However, we can check if revision tended to lead towards more agreement overall. In Table \ref{tab-variance-after-revision}, we show the variance of the entire sample of reported effects post-revision, replacing each researcher's reported task effect with its revision, if they revised their work, and compare variance among the peer-reviewed group to the non-peer-reviewed group. At no point is this difference statistically significant, and it is inconsistent which group had the lower post-revision variance. Taking only the subgroup of actually-revised estimates, we see that these tended to agree with each other more than the group as a whole did in Tasks 1 and 2, but this is also inconsistent, with greater variance than the whole group in Task 3 and when pooling over all tasks (after subtracting by-task means).

```{r}
peer_review_levene |>
  knitr::kable(booktabs = TRUE, 
               caption = 'Post-Revision Variance by Peer Review \\label{tab-variance-after-revision}')
```

In general, the mechanisms by which peer review might be expected to change a researcher's work in normal journal submissions include both that researchers might find peer review comments helpful and incorporate them into their work, and that researchers are required by the journal submission process to incorporate most reviewer comments. In this study, our peer review process can only capture the first of these mechanisms, and in effect may be closer to comments received, for example, during seminar presentations.

@fig-peer-review-effect-distributions shows the distribution of effect sizes estimated by those who did, and did not, engage in peer review in each round. The left column of graphs shows the effects reported in each task before researchers were assigned to peer review, and the right column shows the effects reported in the follow-up task, comparing those either assigned or not assigned to peer review in the previous task. As we might expect given random assignment, effect distributions are fairly similar pre-review between the review and non-review groups, with a slightly narrower distribution of effects for researchers about to be reviewed.

We also see that these groups have very similar effect distributions in their follow-up task. Neither group has a considerably narrower distribution than the other. Levene tests for differences in follow-up round effect variance between the peer-reviewed and non-peer-reviewed groups show p-values of `r number(levene_peer_vs_next_round$levene_p[1], .001)` and `r number(levene_peer_vs_next_round$levene_p[2], .001)` in Tasks 2 and 3, respectively, or `r number(levene_result_pooled[['Pr(>F)']][1], .001)` when pooling the two tasks. This is not strong evidence in favor of the idea that peer review might drive agreement between researchers due to the receipt of feedback.

```{r}
#| label: fig-peer-review-effect-distributions
#| fig-cap: Distributions of Reported Effect Sizes
source('../code/peer_review.R')
p_peer_review_effect_distributions
```

@fig-like-your-reviewer explores the possibility that peer review might not make the peer-reviewed group as a whole more similar, but rather just make someone more similar to their specific reviewer. We calculate the absolute difference in effects between each reviewer pair, in the task they perform before reviewing (left column), in the follow-up task (middle column) and comparing your follow-up task against your reviewer's result this round (right column), with the right column representing the possibility that a researcher may select an analysis so as to produce a result more similar to the one they saw in the previous round. The distributions of absolute differences for non-reviewed researchers are generated as a null distribution by matching every non-reviewed researcher to every other non-reviewed researcher and calculating all absolute differences.[^17]

[^17]: This null distribution represents the distribution of absolute differences among people who did not actually experience peer review. Notably, each non-reviewer is matched multiple times in this approach, instead of just once for reviewers. However, matching the non-reviewers only once to a single random pair just produces a noisier version of this all-matches null distribution. Averaging the single-random-match approach over many random single matches produces the same null distribution.

In @fig-like-your-reviewer we see some of the anticipated effect of peer review for Task 1. Before review (left column), absolute differences between review pairs were more likely to be large than differences between non-review pairs. But by the follow-up in Task 2, both groups were similar, potentially suggesting that peer review reduced the large absolute differences. In the right column, the unreviewed group still shows lower differences, but to a smaller degree. However, none of this holds up in Task 2. Again the actual review pairs started out with larger differences, and those differences shrank for both groups by Task 3, but by the same degree. Appendix Table \ref{tab-peer-review-reg} shows that average absolute differences grew by a statistically significant .051 more for the unreviewed group than for the reviewed group in Task 1, but that this effect reverses to a statistically significant .029 in favor of the unreviewed group for Task 2. This is not consistent strong evidence of peer review making a researcher more like their reviewer as the result of feedback.

```{r}
#| label: fig-like-your-reviewer
#| fig-cap: Comparisons of Effect Sizes vs. One's Reviewer
#| fig-width: 8
#| fig-height: 5
p_more_like_reviewer
```

## Analytic Choices {#sec-analytic}

The next two sections examine the different choices that researchers made, both to demonstrate the variation in ways that different researchers chose to carry out the research tasks, and to relate those choices to differences in outcomes.

Table \ref{tab-estimation-methods} shows the different choices made in estimating the effect of DACA on the probability of employment across all three tasks, in particular the estimator chosen, the use of ACS sampling weights provided by IPUMS, and the choice of standard error adjustment.[^18] The dependent variable of interest, working full-time or not, is binary. However, as is generally standard in applied microeconomics, linear regression was the most common estimator used, with 82% of entries. 13% used logit or probit regression instead. Notably, many reserachers used linear regression as a means of implementing a fully saturated (or nearly fully saturated) difference-in-differences design, in which case the downsides of linear probability models are muted. Other researchers mostly used a matching estimator (sometimes combined with linear regression) or one of several newly-introduced estimators for difference-in-differences designs, like @callaway2021difference.

[^18]: For most researchers, these choices did not change over the tasks, and so we just present the overall view.

The use of sample weights was relatively uncommon, despite their use being advised with survey data like ACS. Only 25% of task completions mentioned the use of weights or any of the standard ACS weight variables.

There was considerable variation across researchers in the selection of standard error adjustment. A slim majority of researchers applied clustered standard errors in some way, but clustered at different levels: state, state/year, or according to a survey clustering indicator like Strata or some other variable. A further 17% of submissions used heteroskedasticity-robust but not cluster-robust standard errors.

```{r}
source('../code/analytic_choices.R')
source('../code/clean_controls.R')
```

```{r}
#| output: asis

base %>%
  mutate(se_adjustment = factor(se_adjustment, levels = c(
    'Cluster (State)',
    'Cluster (State & Year)',
    'Cluster (ID/Strata/Other)',
    'Het-Robust',
    'Other/Bootstrap',
    'None'
  ))) %>%
  filter(!(round %like% 'Revision')) %>%
  sumtable(vars = c('method','Weights','se_adjustment'), 
         labels = c("Method",'Weights','S.E. Adjustment'),
         title = "Estimation Methods",
         col.breaks = 2,
         note = '\\begin{tabular}[x]{@{}r@{}}This table shows details on estimation, not research design. "Difference-in-differences" \\\\ implemented with linear regression, for example, counts here as linear regression.\\end{tabular}',
         out = 'latex',
         anchor = 'tab-estimation-methods')
```

Table \ref{tab-controls-across-rounds} shows the average rate of inclusion of covariates across all three tasks. These can be read as the share of researchers who included the covariate, with the exception of "Other", which allows each researcher to have multiple "Other" controls. The most common covariates included are shown, with the exception of indicators for "eligible for DACA" or "in a post-DACA period", as these are considered part of the core research design rather than covariates. Variables are included here regardless of the functional form used to include them.

The most common included controls were for state, year, age, and sex, which were included as covariates for more than 50% of researchers in all three tasks. However, there was heavy variation in the sets of included covariates. In Task 1, for example, there are ten covariates with rates between .2 and .8, meaning that at least 20% of the researchers made a different decision on inclusion of the covariate than the majority. There are four covariates in the 40-60% range, meaning that the researchers were almost evenly split on whether or not to include the covariate. These rates did not change much by Task 3.

Across all rounds, in which there were `r 3*145` submitted research tasks, there were `r exact_matches[, .N]` different unique sets of included covariates after "Other" covariates are excluded. `r percent(exact_matches[numpairs == 1, sum(numpairs)]/sum(exact_matches$numpairs))` of submissions had a set of covariates that was unique for the task. `r percent(exact_matches[numpairs == 2, sum(numpairs)]/sum(exact_matches$numpairs))` shared a covariate set with one other person in that task, `r percent(exact_matches[numpairs %in% c(3,4), sum(numpairs)]/sum(exact_matches$numpairs))` shared with two or three other people, and only the those with no controls shared with more than three other people.

```{r}
# Average appearances across rounds
controls_across_rounds |>
  knitr::kable(booktabs = TRUE, 
               caption = 'Average Rate of Covariate Inclusion Across Rounds \\label{tab-controls-across-rounds}')
```

There was very little agreement across researchers on the exact set of appropriate controls, or the inclusion or exclusion of any given control (aside from those very rarely included). Did these choices impact the effect estimates? Not by much. Table \ref{tab-effects-by-controls} shows the average effect among researchers who included a given control, pooling all three tasks. The mean reported effects differ by only .023 percentage points comparing the covariate included in analyses with the highest average effectestimates (Continuous Years in the USA) against the lowest (Race). This likely overstates the impact of covariate selection here, as selecting the highest vs. lowest after estimates are known will bias us towards a larger difference from noise alone.

There do not appear to be major differences in the average reported standard errors either, or in the standard deviation of the effect distribution among reserachers including that covariate.

```{r}
effects_by_controls |>
  knitr::kable(booktabs = TRUE,
               caption = 'Estimated Effects by Control-Variable Inclusion \\label{tab-effects-by-controls}')

```

While the inclusion of a given common control variable does not strongly predict an estimated effect, in Table \ref{tab-effects-by-functional-form} we look at the most common covariates and examine whether their functional form meaningfully affects the estimated effect. The selection of functional form explained more variation in average estimated effects than the inclusion of covariates did, at least in this context. For both age and the State/Year controls, the difference between the highest-average-effect functional form variants and the lowest, in both cases comparing a linear control against a fixed effect, was greater than the difference between highest and lowest among covariates included.

```{r}
trans_con[, .(N = uniqueN(paste0(Q1,Round)),
                Effect = number(mean(Effect, na.rm = TRUE), .001),
                `Mean SE` = number(mean(SE, na.rm = TRUE), .001),
                `Effect SD` = number(sd(Effect, na.rm = TRUE), .001)),
            by = .(Category = category, Control = Relabel)][order(Control)] |>
  knitr::kable(booktabs = TRUE,
               caption = 'Estimated Effects by Functional Form of Control Variable \\label{tab-effects-by-functional-form}')
```

These specific findings about the impact of choices on effects - that the inclusion of different covariates did not have a major impact on estimated effects, or that the choice of functional form had a greater impact than the selection of covariates - should not be expected to generalize, and is specific to this research task. However, what we can take from this section is that there is substantial variation across researchers in what they believe the appropriate set of covariates to be and, for a given covariate, what the appropriate functional form is. We can also see that, in the case of this particular study, these decisions, while varied, did not fully explain the variation in effects between researchers.

## Sample Limitations {#sec-sample-limitations}

In this section we examine the ways in which researchers defined their analytic samples, as well as defined the treated group, and any comparison group included in the data analysis that was not treated. These data are derived from researcher responses to a survey about their work, in which they were asked to describe how they limited the size of their sample before downloading it from IPUMS and which variables they used to further drop observations from the sample before analysis. For example someone might say that they only kept observations for which HISPAN == 1 (the observation is Hispanic-Mexican), along with other restrictions. These two responses are combined to define the full analytic sample.

Researchers were also asked to describe how they defined the treated group who was affected by DACA. For example someone might say that only those with CITIZEN == 3 (non-citizens) were eligible, along with other restrictions. These were combined with the analytic-sample restrictions to make the full treated group definition. Finally, researchers were similarly asked to describe the conditions that defined someone who would be included in analysis but not be treated by DACA, which were combined with the analytic-sample restructions to make the full comparison group definition.

Researchers were asked to specify these conditions as precisely as possible to match what they did, and to use IPUMS variable names in their descriptions. The project organizers coded these into a set of boolean conditions defining the overall sample, treated group, and comparison group for each researcher, in some cases reviewing the code directly or asking researchers to clear up uncertainty where survey responses were unclear, but in general taking the researcher survey responses at their word. For these analyses, Task 3 is omitted because the analytic sample is defined for all researchers.

```{r}
source('../code/sample_limitations.R')
```

Table \ref{tab-number-of-sample-limitations} looks purely at the number of distinct variables referenced in the sample limitations, regardless of what they are. In Task 1, the typical researcher used five variables to define their analytic sample, and an additional four to define their treated group. In Task 2, where inclusion criteria were shared, both of these numbers increased, but there was still considerable variation, with the 25th and 75th percentiles using 3 and 10 variables to define their full sample. Definition of the treated group was more shared, with the 25th and 75th percentiles using 9 and 12 variables, respectively.

```{r}
#| output: asis
# Basic sample limitations table
sumtable(basic_samp_limitations, vars = c('Whole Sample','Treated Group','Untreated Group'), add.median = TRUE, group = 'Round', group.long = TRUE, title = 'Number of Variables Referred to in Sample Limitations', anchor = 'tab-number-of-sample-limitations',
         out = 'latex', numformat = formatfunc(digits=2, nsmall = 1))
```

As for how those variables are actually used, Table \ref{tab-sample-limitations-extensive} shows how these variables were implemented as sample restrictions. Importantly, these reflect the survey responses given by researchers. Some of the "none" responses in Task 2 in particular reflect researchers who did use the variable in some way but did not report it in their description of results.[^19]

[^19]: Other notes of interest for reading the table: (a) "Multistep condition" refers to cases where the variable is included, but only as a part of a complex boolean statement involving many variables. These most commonly appeared in definitions for the comparison group, which are not in the table, in the format of "fails any one of the following set of DACA eligibility requirements." and (b) for Education/Veteran status, recall that the mention of this eligibility requirement was omitted from the Task 1 instructions, which explains why very few researchers used these variables to define their samples or treated groups in Task 1.

We see a huge amount of variety in the ways these variables were used, including in Task 2 for the treated-group definition, where there is a correct answer according to the instructions (and similarly a correct answer for some variables in the Task 1 treated-group definition).[^20] For each variable, the most-common option, listed at the top, is the "correct" answer for defining the treated group, with two exceptions: (a) for Citizenship, there is a second justifiable answer in "Non-Citizen or Naturalized After 2012." These immigrants would have been eligible for DACA in 2012, but would not be eligible for DACA as of the time they were surveyed, so they would have received a partial "dose" of DACA, which could justifiably be included or excluded, and (b) for Years Continuous in USA, where DACA guidelines require that the immigrant have lived *continuously* in the United States for five years as of 2012. Most researchers used only year of immigration being before 2007 to satisfy this criterion, but others used the YRSUSA set of variables which specifically track living continuously in the country.

[^20]: Keep in mind also that these are recoded versions of the actual survey submissions sent in by researchers. The survey question asked respondents to use IPUMS variable names to describe their choices. So for example the individuals reporting that they used only high school graduates or *non-veterans,* instead of veterans as per the instructions, likely did not intentionally choose to use non-veterans and write "I chose to use non-veterans in my sample" in their response but rather they wrote "VETSTAT == 1," which indicates "non-veteran", perhaps based on a misunderstanding of the IPUMS documentation (veterans are VETSTAT == 2).

For all other variables besides Years Continuous in USA, the option matching the instructions was the most common, but we also see plenty of variation. We also see considerable variation for the columns in which there is not a clear "correct" option, like the analytic sample definition. No single way of applying any variable was used by more than 84% of the sample in any case. One interesting feature is the use of both "\< 16" and "\<= 16" for age at migration, and "\< 2007" and "\<= 2007" for year of migation. For year of migration, the two are similarly popular. Also interesting is the distinction between researchers using age defined in years to determine eligibility vs. age defined in quarters, which makes a difference given that eligibility is based on age specifically in June 2012.

```{r}
#| output: asis
# Variables used in limitations
sumtable(r12samp, titles, group = 'Round/Sample',
         out = 'latex',
         fit.page = '.75\\textwidth',
         anchor = 'tab-sample-limitations-extensive',
         note = 'Multistep condition means the variable is one part of a complex boolean involving many different variables.',
         title = 'Sample Restriction Methods')
```

Showing the impact of these choices on estimated effects is difficult since, aside from the most-common option, any specific alternative does not have enough people using it to make a reasonable comparison. However, we show estimated effects and, for analytic-sample restrictions, analytic sample size by sample limitation choice in Appendix Tables \ref{tab-task1effectsample-full} for Task 1 and Table \ref{tab-task2effectsample-full} for Task 2. There are large differences in estimated effects and sample sizes across many of these different sample restriction choices, but in many cases these comparisons are based on very small samples.

The two comparisons for which an alternative was common enough to compare are for the YRSUSA inclusion and the use of "\< 2007" vs. "\<= 2007" for year of migration, which are shown in \ref{tab-task1effectsample} for Task 1 and Table \ref{tab-task2effectsample} for Task 2. For both of these, the relationship between these choices on effects varies from negligible to a several percentage-point difference associated with a single sample restriction change, a fairly minor one in particular for "\< 2007" vs. "\<= 2007". Effect differences are larger in Task 2. However, in Task 1, even though estimated effects are similar, sample sizes are considerably larger for the less-restrictive option, and so reported standard errors would be lower, and statistical significance more likely.

```{r}
#| output: asis
# sample limitations and effects/samples, task 1
cat(dftoLaTeX(make_efftab(r12samp[Round == 'Task 1'])[Variable %in% c('HEADERROW', 'Year of Immigration','... < 2007','... <= 2007','Years Continuous in USA','... Used YRSUSA','... No YRSUSA')],
          align = 'llllllllll',
            title = 'Task 1 Effect and Samples by Sample Definitions',
            anchor = 'tab-task1effectsample',
            fit.page = '\\textwidth'))
```

```{r}
#| output: asis
# sample limitationsand effects/samples, task 2
cat(dftoLaTeX(make_efftab(r12samp[Round == 'Task 2'])[Variable %in% c('HEADERROW', 'Year of Immigration','... < 2007','... <= 2007','Years Continuous in USA','... Used YRSUSA','... No YRSUSA')],
          align = 'llllllllll',
            title = 'Task 2 Effect and Samples by Sample Definitions',
            anchor = 'tab-task2effectsample',
            fit.page = '\\textwidth'))
```

## Researcher Characteristics and Effects {#sec-researcher-chars}

In this section we evaluate the relationship between researcher characteristics and the effects they reported. As in our preregistration, analysis in this section is performed in a muliple-analysts style, with the two project organizers taking the same data and research question and performing independent analyses.

### Analysis by Project Organizer A

For each categorical researcher characteristic specified in @sec-sample-characteristics, as well as an indicator for the use of R or Stata as a programming language. In each case, categories with 5 or fewer researchers in them were omitted before performing the analysis. Table \ref{tab-orga-effects} shows the F-statistic from a regression of the reported effect estimate on a set of indicators for that characteristic, as well as the associated $p$-value and $R^2$ from that regression. This table allows us to see whether researchers with different characteristics reported different effect levels. Table \ref{tab-orga-deviation} does the same, but uses absolute deviation from the sample mean as the dependent variable. This table allows us to see whether researchers with different characteristics showed greater agreement on effect levels with the group as a whole.

```{r}
res_tab_effect |>
  knitr::kable(booktabs = TRUE,
               caption = 'Predicting Effect Level with Researcher Characteristics\\label{tab-orga-effects}')
```

```{r}
res_tab_deviation |>
  knitr::kable(booktabs = TRUE,
               caption = 'Predicting Effect Deviation with Researcher Characteristics\\label{tab-orga-deviation}')
```

Tables \ref{tab-orga-effects} and \ref{tab-orga-deviation} show that researcher characteristics hold basically no explanatory power for estimated effects either in level or deviation from the mean. Nearly all $p$-values are well above .05. In \ref{tab-orga-deviation}, the $p$-value for race as an explanatory variable in Task 1 had a $p$-value below .1, but given how many comparisons there are here, this is likely to just be noise.

The only researcher characteristic that did seem to matter was the choice of programming language, which only weakly predicted effect level, but was a statistically significant predictor of being close to the mean effect in all three rounds.

```{r}
#| fig-cap: Deviation from Sample Mean of Reported Effect by Language
#| label: fig-deviations-by-language
#| fig-width: 8
#| fig-height: 6
p_deviations_by_language

# NOTE FOR WRITEUP: Of the big R outliers two people were in that category every round, and everyone else was only in there once.
```

@fig-deviations-by-language goes further into the split by language. We see that, of the two languages, Stata users were more likely to report effect estimates near the sample mean. 6.4%, 1.8%, and 0.9% of Stata users were more than .1 in absolute distance from the sample mean in Tasks 1, 2, and 3, respectively, while for R those values are 15.6%, 9.4%, and 12.5%. The number of R users is relatively low at 32,[^21] and so these numbers are sensitive to any researchers who were consistently outliers. There were two R users who had an absolute deviation from the mean of .1 or more every round, while all other R researchers with deviations of .1 or more only had deviations that large in a single round. If we omit those two consistently-high-deviation R users, the percentages are 10%, 3.3%, and 6.7% for R users, which are still higher than the percentages for Stata users.

[^21]: This is one lower than the value reported in @sec-sample-characteristics because the researcher who was dropped from analysis, mentioned later in @sec-sample-characteristics, was an R user.

Overall, there is little role for researcher professional or demographic characteristics in predicting either the level of the effects they reported, or the deviation of those effects from the mean. There is some explanatory power for the choice of programming language. R users were more likely than Stata users to report estimates far from average of what other users reported.

### Analysis By Project Organizer B

Table \ref{tab-researcher-characteristics-regs-b} looks at within-researcher variation in effect estimates across tasks. In the first three columns, the dependent variable is the absolute difference in effects for a given researcher across two tasks, while in the fourth column, the dependent variable is a researcher's maximum estimated effect minus their minimum.

```{r}
modelsummary(researcher_characteristics_models_b, 
             coef_map = c(
               '(Intercept)' = 'Intercept',
               'factor(position)Faculty' = 'Faculty',
               'factor(position)Grad student' = 'Grad student',
               'factor(position)Uni researcher' = 'Uni researcher',
               'factor(position)Other' = 'Other',
               'factor(position)Private researcher' = 'Private researcher',
               'factor(position)Public researcher' = 'Public researcher',
               'factor(degree)PhD' = 'PhD',
               'factor(degree)Not PhD' = 'Not PhD',
               'factor(experience)6+ papers' = '6+ papers',
               'factor(experience)1-5 papers' = '1-5 papers',
               'factor(experience)0 papers' = '0 papers'
             ),
             stars = TRUE,
             output = 'kableExtra',
             title = "Model Coefficients and Standard Errors for Task Comparisons\\label{tab-researcher-characteristics-regs-b}",
             gof_omit = '^(?!R2|Num.Obs)',
             escape = FALSE
)
```

Most researcher characteristics do not predict absolute within-researcher variation. Career stage, occupation, and number of published papers do not predict absolute differences in estimates across tasks to a statistically significant degree, with few exceptions. 

One exception is that private researchers saw larger absolute changes between Task 1 and Task 3, and also more absolute variation overall, although the latter is only significant at the $\alpha = .1$ level. Probably the most interesting is that inexperience was related to smaller changes from Task 1 to Task 3: those who do not have a PhD showed a smaller change between Task 1 and Task 3 (significant at $\alpha = .1$), and those with fewer papers also showed smaller absolute changes than those with 6+ papers (insignificant).

### Two-Analyst Results for Researcher Characteristics

Both project organizers used the same data to answer the same research question. From the preregistration: "Both primary authors will, independently, analyze the relationship between (a) researcher characteristics and reported research results in earlier stages, and (b) attrition from the study and reported research results in later stages." Because there was so little attrition from the study after Task 1, part b was dropped from the analysis. While this question is much more open-ended than the main causal inference question in this paper, it is notable how different the two approaches were. The dependent variable of interest differed, as did the analytic method and selection of relevant predictors. Both organizers found, however, that researcher characterisics were not strong predictors of what the researchers did.


## Bimodality in the Task 2 Effect Estimates {#sec-bimodal}

One of the surprising results in @sec-variation was the effect distribution in Task 2. In designing the study, we had expected that each task would show a narrower distribution of effects than the previous task. While we did generally see this pattern for sample sizes and some researcher choices, the distribution of effects in particular became wider going from Task 1 to Task 2. We also saw some emerging bimodality, where the larger part of the sample reported estimates that reflected the distribution of effects already seen in Task 1, while a smaller group of researchers reported larger effects that were more like those found in Task 3. In this section we explore possible explanation for the unexpected findings in Task 2.[^22]

[^22]: This section is entirely un-preregistered, as we did not anticipate this finding.

The fact that the effect distribution is not just wider but rather gathers at a high and a low point makes the task of explaining it somewhat easier, as we can look for features that predict reporting a higher or lower estimate.

Several anticipated correlates did not explain the bimodal outcomes of Task 2. @fig-effect-vs-sample and @fig-effect-vs-se in the Appendix show that the Task 2 reported sample sizes and standard errors do not strongly explain the effects reported. Recalling @sec-analytic, there were not enormous differences in reported estimates by control variables included, and there were not covariates associated with large effects that increased in prominence in Task 2, so it is unlikely that changes in covariate choice or importance explain the effect distribution.

```{r}
source('../code/round_2_bimodality.R')
```

A possible expanation of the bimodality is that some researchers found their Task 1 analyses or results "sticky" and tried to match them too closely, while researchers who did not attempt to do this instead produced results like those in Task 3, since the Task 2 instructions are very similar to Task 3. However, @fig-task-1-vs-2 shows effectively no relationship betwen a given researcher's Task 1 estimate and their Task 2 estimate.

```{r}
#| label: fig-task-1-vs-2
#| fig-cap: Estimates Effects in Task 1 vs. Task 2
p_task1_vs_task2
```

```{r}
# conchange[, .(Increase = percent(mean(R2Effect > .05, na.rm = TRUE))),by = .(Control, ControlSwitch)] |>
#   dcast(Control ~ ControlSwitch) |>
#   knitr::kable(booktabs = TRUE)
```

```{r}
# conchange[, .(N = .N),by = .(Control, ControlSwitch)] |>
#   dcast(Control ~ ControlSwitch)
```

We then look at sample limitations and definitions. Table \ref{tab-sample-definition-changes} uses the sample-restriction coding from  @sec-sample-limitations, and examines whether a given researcher changed their use of a given variable in their sample restrictions from Task 1 to Task 2. This could be any change, for example going from not using the "Hispanic" variable at all to using it, or switching from "Hispanic-Mexican" to "Hispanic-Any". The table shows that, with Hispanic and Birthplace as exceptions, the share above .05 tended to be higher among researchers who changed the way they used a given variable in defining their sample.

```{r}
numberp1 = label_number(accuracy = .01)
changesamp[, .(N = .N, Increase = numberp1(mean(Increase, na.rm = TRUE)),
               `Standard Error` = numberp1(median(`Standard Error`, na.rm = TRUE)),
               R2Effect = numberp1(mean(R2Effect, na.rm = TRUE)),
               Abovep05 = percent(mean(R2Effect > .05, na.rm = TRUE), .1)),
           by = .(`Sample Limitation`, Changed = fifelse(Changed, 'Yes','No'))][order(`Sample Limitation`, Changed)][!is.na(Changed)] |>
  knitr::kable(booktabs = TRUE, caption = 'Changes in Sample Limitations from Task 1 to 2, and Effect Changes \\label{tab-sample-definition-changes}')
```

Taking the explanatory power of sample restrictions, we then look at the treated-group definition. Task 2 gave a very precise definition of who should be included as a part of the treated group. We examine whether a given researcher followed the full set of treated-group definition instructions precisely or not. The mismatch could be small, such as using "\<= 16" instead of "\< 16" for age at migration, or large, such as omitting that eligible people must be non-citizens. In @fig-match-vs-mismatch we show their distribution of effects against researchers who had a mismatch in their criteria in any way. The graph shows that the bimodality heavily driven by the group that precisely matched the treated-group definition. This implies that the bimodality in Task 2 may be explained in large part by a split between researchers who exactly followed the instructions, and so effectively matched what a typical researcher found in Task 3, and those who did not. This does not fully explain researcher behavior: note that there is also a weight of researchers with higher results who did not match perfectly, and also that much of the density of the perfect-match group is at Task 1 effect levels, but keep in mind that there are many other decisions in analysis to be made, and this captures only one angle where determining the correct decision is easiest.

```{r}
#| label: fig-match-vs-mismatch
#| fig-cap: Task 2 Effect Distributions Among Those with Exact Treated-Group Definition Matchs vs. Those with Some Mismatch
p_match_vs_mismatch_distribution
```

Further, Table \ref{tab-match-by-field} shows that the share of researchers matching exactly is fairly low, between 20-25% by field, keeping in mind that even very minor mismatches are counted as mismatches. Further, perfect-match rates were slightly higher among researchers whose work was closest to the field that the research task was in, immigration and labor, although this difference was not statistically significant at the 95% level.

```{r}
rfield |>
  knitr::kable(booktabs = TRUE,
               caption = 'Share of Researchers Matching Treated-Group Definition Exactly by Field\\label{tab-match-by-field}')

# our t-test
# d = data.table(field = c(rep(TRUE,1+3+12+35),rep(FALSE,c(18+76))), out = c(rep(TRUE,13),rep(FALSE,3+35),rep(TRUE,18),rep(FALSE,76)))
# t.test(d[field == FALSE, out], d[field == TRUE, out])
```

# Conclusion

## Recommendations for Improved Practice

What do these results imply should change about the practice of applied microeconomic research?

To some degree, the findings of this paper do not reflect a problem to be solved. The fact that different researchers approach a problem differently is not in itself a problem, as long as any points of disagreement are visible to the reader and subject to scrutiny and disagreement, and the reader understands that a given study or set of research decisions is not the last word.

However, there is a problem to be solved to the extent that researcher variation reflects either (a) error, or (b) choices that are unexamined or invisible while also being something that researchers would choose differently.

In this study, we found considerable variation across researchers in the approaches taken to answering the main question in Task 1, which most closely reflects actual practice. While the distribution of effects did not considerably narrow from Task 1 to Task 2, the sample sizes did, as did the treated and comparison group definitions. Although we generally did not reject Levene tests of equal variance across rounds, descriptively there was an obvious narrowing of the distribution of effects. There was, in both rounds, a lot of variation in the set of covariates included, as well.

To the extent that these choices reflect research design and modeling choices, the problem is somewhat already addressed. Researchers are used to critiquing research design and modeling choices in public work. Because of this, we would expect that all of these choices would be reported in a writeup of research, where they could be critiqued. What would not typically occur is someone actually testing whether many of these alternate choices actually lead to different results, especially for seemingly more innocuous choices like covariate functional form, which was found in this study to be more consequential than the set of covariates included.

On the part of researcher practice, this set of results suggests the use of multiverse analysis [@steegen2016increasing], where the researcher considers every combination of reasonable modeling decisions and demonstrates their effects on estimated effects, or even many-analysts approaches to producing original work, as we did in @sec-researcher-chars. On the part of journals, this suggests that journals should consider accepting work that is a variation in the approach to a published work, even if that variation is not framed as a replication or rejection of the original study, currently a barrier to the publication of replications [@galiani2017incentives].

However, this study did not find that research design and modeling choices were the majority of explained researcher variation. Instead, this came in the form of data cleaning and preprocessing, including the selection a sample and the creation of variables indicating the treated group. Some of this variation could be classified as error, for example researchers in @sec-bimodal whose treated-group definition in Task 2 did not match the instructions. Other parts of this variation could be reasonable disagreement.

That much of the relevant variation seems to come along the lines of data cleaning and preprocessing is possibly unsurprising, given how this task is currently handled in economics. Relative to modeling and research design, data cleaning and preprocessing receive little attention. It is common for minor, or even important, details of data processing to be left of the description of methods in published papers. Data cleaning is also not formally taught in most graduate programs. A professor who would never allow a research assistant to decide their research design or model might be happy passing along the data cleaning task to an assistant, even though, as this paper shows, the task may be just as relevant to the results and just as prone to arbitrary choice-making.

Because data cleaning gets so little attention, it is perhaps to be expected that we saw the most variation here. Without formal training in PhD programs, or a culture of reviewing and critiquing data cleaning and preprocessing in research papers, there is little opportunity for researchers to *learn to do the same thing*, and so we see heavy variation. Contrast this to the popular use of linear probability models in Table \ref{tab-estimation-methods}, for example, which is common in applied microeconomics, especially in difference-in-differences designs. Because its use is very visible, researchers can see that it is the standard in the field. Whether or not it is actually the best method to use here, it is a method that researchers know has been agreed upon in the field. We see a similar story play out with standard error adjustments in the same table. The standard method by which economists learn data processing is on a much narrower scale, often from one's advisor or from others on a small research team.

This problem implies several possible policy solutions. Among broader system-wide changes, the introduction of data cleaning and preprocessing classes teaching methods and best practices in the standard PhD applied economics curriculum would likely improve the quality of economics research as well as reduce researcher variability. This presumes the existence of a set of best practices, or at least standard practices. So, an attempt should be made to codify and popularize a set of data-cleaning best practices, similar to how applied economists routinely learn about modeling best practices (for example any number of econometrics textbooks, or in the applied literature papers like @abadie2023should). Other fields have already made strides in this direction [e.g. @osborne2012best; @jafari2022hands] so this effort would not need to start from scratch, but would be improved by designing recommendations most relevant to an applied microeconomics context.

Those recommendations may not be in the control of any one researcher, though. The policy that this paper implies for individual researchers and journal editors or reviewers is that economics as a field should consider data cleaning and preprocessing to be just as much a part of the methods as the choice of model. Researchers should fully describe their data cleaning processes in their papers to the same level of detail that they describe their modeling choices. They should also perhaps subject any arbitrary data-cleaning decisions to multiverse analysis as well (note that this is also a suggestion of @steegen2016increasing). Further, an increasingly common requirement of journal submissions is to provide a replication package of the code used to perform a paper's analyses (for example @aeaguidelines). However, these replication packages often begin from an already-prepared data set, and only include the code necessary to run models. It would be advisable to include data preprocessing code in these replication packages.

## Discussion

This paper describes the results of a large many-analysts project in applied microeconomics. We found large amounts of variation in the choices made by researchers, especially in regards to data cleaning and processing, research design, the definition of treated and comparison groups, and the selection and functional form of controls. Some of this variation appears to be from researcher data cleaning processes that do not match the instructions, derived from policy realities, for constructing the treated group. Variation was not strongly constrained by the influence of peer review or a shared research design, but there was a (descriptive if not statistically significant) reduction of variation when researchers were provided with pre-cleaned data.

Interestingly, we do not find huge amounts of variation in actual estimated effects of the policy. There were some outliers, and statistical significance varied between researchers. However, the central range of estimated effects of DACA on the probability of working full time effects generally was not very wide, with the difference between the 25th and 75th percentiles typically only 2-3 percentage points. However, the fact that widely different sample definitions and modeling choices led to a narrow range of effects is not guaranteed to generalize to other contexts.

There were also parts where researchers behaved very similarly. The use of linear regression modeling was very popular, and very few researchers used unadjusted standard errors, although the specific adjustment or clustering level varied. 

What we might learn from this study, and perhaps the wider world of studies on researcher variation, is perhaps obvious: in cases where there were well-acknowledged "standard" ways of doing something, like using linear modeling in a difference-in-differences-type setting with a binary outcome, or adjusting one's standard errors, researchers tended to do that thing. And where there was no well-acknowledged standard, like in the choice of clustering level, the selection of covariates in this particular setting, or in data cleaning, researchers behaved differently, sometimes to consequence and sometimes without it mattering much.

While researcher error also plays a part, the impact of the absence of standards is an emergent result from this study. Readers and practitioners of research should expect arbitrary variation in parts of research, like data cleaning, that do not have standards. 

The development of best-practice standards in areas where we currently do not acknowledge them would be likely to improve applied microeconomics towards being a more mature, sophisticated, and believable field than it is today. We have highlighted data-cleaning practices as being an especially fruitful place to develop these standards, but the same applies in other areas as well like the level of clustering (an example of a place where development of consensus guidance is already underway in @abadie2023should). The optimal level of researcher variation is not zero, as individual researchers often have good reasons not to match the methods and practices used by others. But when this occurs, it should be because there *is* a good reason to deviate from the template, rather than because we have no template to begin with.

# References {.unnumbered}

::: {#refs}
:::

\FloatBarrier

\appendix

# Appendix {.unnumbered}

```{r}
names(peer_review_reg) = c('Task 1','Task 2')
peer_review_reg |>
  msummary(stars = c('*' = .1, '**' = .05, '***' = .01),
                         gof_omit = c('IC|R2|RMSE|Err'),
           coef_rename = c('Intercept',
                           'Comparison: Next Round',
                           'Comparison: Next Round vs. This Round',
                           'Unreviewed',
                           'Next Round x Unreviewed',
                           'Next vs. This x Unreviewed'),
           output = 'kableExtra',
           title = 'Paired Absolute Effect Differences and Peer Review \\label{tab-peer-review-reg}', escape = FALSE)
```

```{r}
#| label: fig-effect-vs-sample
#| fig-cap: Task 2 Effect Size and Sample Size
p_sample_size_scatter
```

```{r}
#| label: fig-effect-vs-se
#| fig-cap: Task 2 Effect Size and Standard Error
p_standard_error_scatter
```

```{r}
#| label: tab-share-above-p5-by-control
# shtask |>
#   knitr::kable(booktabs = TRUE,
#                caption = 'Share of Effects Above .05 by Covariate Included')

```

```{r}
#| output: asis
# sample limitations and effects/samples, task 1
cat(dftoLaTeX(make_efftab(r12samp[Round == 'Task 1']),
          align = 'llllllllll',
            title = 'Task 1 Effect and Samples by Sample Definitions, Full View',
            anchor = 'tab-task1effectsample-full',
            fit.page = '\\textwidth'))
```

```{r}
#| output: asis
# sample limitationsand effects/samples, task 2
cat(dftoLaTeX(make_efftab(r12samp[Round == 'Task 2']),
          align = 'llllllllll',
            title = 'Task 2 Effect and Samples by Sample Definitions, Full View',
            anchor = 'tab-task2effectsample-full',
            fit.page = '\\textwidth'))
```

