---
title: "The Sources of Researcher Variation in Economics"
author:
  - id: HK1
    number: 1
    name: Nick Huntington-Klein
    email: nhuntington-klein@seattleu.edu
    phone: 1-206-296-5815
    fax: NONE
    orcid: 0000-0002-7352-3991
    degrees: PhD
    attributes:
      corresponding: True
    affiliations:
      - id: NHK1_1
        number: 1
        name: Seattle University
        department: Department of Economics
        address: 901 12th Ave, Seattle, WA, 98122
  - id: CP2
    number: 2
    name: Claus Pörtner
    email: cportner@seattleu.edu
    phone: 1-206-296-2593
    fax: NONE
    orcid: 0000-0001-8052-9462
    degrees: PhD
    attributes:
      corresponding: True
    affiliations:
      - id: CP1_1
        number: 1
        name: Seattle University
        department: Department of Economics
        address: 901 12th Ave, Seattle, WA, 98122
format: 
  pdf:
    keep-tex: true
editor: visual
bibliography: References.bib
execute:
  warning: false
  message: false
  echo: false
acknowledgements: This project was supported by the Alfred P. Sloan foundation grant G-2022-19377. Many thanks to Kian Farzaneh, Amrapali Samanta, and Erica Long for research assistance, to the researchers Mira Chaskes, Jennifer A. Heissel, Elaine L. Hill, Rajius Idzalika, Joshua D. Merfeld, and Ethan Sawyer, who contributed but did not want an authorship slot, to the researchers who wished to remain anonymous, and to the researchers who enlisted in the study but were not eligible or were not able to complete all three rounds of the
include-in-header:
  text:
    \usepackage{booktabs}
    \usepackage{longtable}
output: 
  rticles::arxiv_article:
    keep_tex: true
---

```{r}
{
  library(rio)
  library(data.table)
  library(ggplot2)
  library(nicksshorts) # remotes::install_github('NickCH-K/nicksshorts')
  library(stringr)
  library(scales)
  library(vtable)
  library(fixest)
  library(modelsummary)
  library(here)
}

dat = import("../data/cleaned_survey_post_corrections.parquet", setclass = 'data.table')
dat[, Revision_of_Q14 := str_replace_all(Revision_of_Q14, '‚Äì','-')]
dat[, Revision_of_Q17 := str_replace_all(Revision_of_Q17, '‚Äì','-')]
dat[, Revision_of_Q20 := str_replace_all(Revision_of_Q20, '‚Äì','-')]

# CHANGE THIS COLOR PALETTE TO CHANGE ALL GRAPHS
colorpal = palette.colors(palette = 'Paired')
```

# Introduction

The social and behavioral sciences produce a staggeringly large flow of empirical results. A responsible reader of this literature should wonder how much they can trust a given study, given the potential for errors, fluke results, or intentional attempts to produce a specific result. Any responsible producer of this literature, however, is well aware that even a researcher doing their best to avoid these problems must make hundreds of choices in the process of collecting and cleaning data, planning their estimation, and coding their analysis, in other words "researcher degrees of freedom" [@simmons2011false]. Even if Researcher A's choices would stand up to scrutiny and argument from a reviewer or reader, Researcher B with the same goal, data, and skills might have reasonably chosen in a different way that would also stand up to scrutiny. If A and B's choices lead to different results, but only one of them performs the study, then this is a source of largely arbitrary variation in the collection of published results. Estimates in one context suggest this variation might outweigh the population variation we typically consider when estimating standard errors [@holzmeister2023heterogeneity].

This study looks at the impact of researcher degrees of freedom on results, and also attempts to isolate researcher degrees of freedom at different stages of the research process to try to isolate where researcher choice is most varied, and most strongly influences results. We do this using a "many-analysts" design where multiple researchers attempt the same research task, specifically a task that is common across applied econometrics: estimating the causal effect of a policy that is implemented at a specific period of time and affects some people but not others. We look at differences between researchers in the results as well as the analytic and data cleaning choices made.

We expand on a typical many-analysts design by introducing multiple iterations of analysis, each time restricting the amount of choice that researchers can make and so reducing researcher degrees of freedom. This allows us to observe the overall amount of variation in estimates between researchers, as is common in many-analysts designs, and also to separately evaluate the influence of choice in research design and in data cleaning, as well as the impact of peer review.

We find meaningful differences in the ways that different researchers approach the same research task. Some of these differences come from decisions that would receive a typical and expected amount of scrutiny from a reader, like the research design, and choice of control variables. Other differences came from sources where researchers did choose differently but a reader might not recognize that a consequential decision had been made, like in the functional form of the control variables or on a number of data cleaning or sample limitation decisions. When researchers were forced to all use the same research design, results became more similar, especially among the researchers most familiar with the subfield of the research task. Researcher agreement increased sharply when pre-cleaned data was provided to researchers, implying that data cleaning decisions are a major source of variation between researchers. Development of more mature and standardized data cleaning procedures, and increased visibility for data cleaning, may have a meaningful impact on the consistency and believability of results in applied microeconomics.

## Previous Work on Research Reliability and Researcher Degrees of Freedom

In economics, suspicion about empirical results is not new [@leamer1983let]. The most recent wave of concern inspired by discussions, originating in the field of psychology, of the "replication crisis", which shows that a high percentage of studies cannot be replicated when tested using new data [for example @open2015estimating or @camerer2016evaluating in economics], that study code and data is not available or does not reproduce the published results [@herbert2021reproducibility], or that "policing replications" that test sensitivity of published results are rare [@ankel2023economists].

However, this greater body of replication work takes an existing study as a baseline and asks whether it is robust to re-evaluation in some way. Questions about researcher degrees of freedom are not about whether a given study can be challenged, but whether a different researcher performing the same study would have done it differently if they had been the person to perform it, without taking the original as a baseline.[^1] There is some research on this topic in regards to researcher identity or political orientation, as in @jelveh2024political, or personality characteristics, as in @sulik2023scientists. But a common way to empirically study researcher degrees of freedom is using a many-analysts design.

[^1]: Notably these two fields intersect, and some failures to replicate in replication studies may be due to researcher degrees of freedom, where both the original study and the replication made reasonable choices but found different results [@bryan2019replicator]. The difference in framing here is that the replication literature views the differing choices as a challenge to the validity of the original results, while the researcher degrees of freedom framing views both as part of a universe of reasonable results, assuming both analyses are defensible.

The many-analysts design,[^2] popularized by @silberzahn2018many, gives the same data set to multiple teams of researchers and have them independently try to answer the same research question.

[^2]: Many-analysts designs are sometimes referred to as "crowdsourced" science.

Many-analysts studies have now been carried out in many fields, including microeconomics [@huntington2021influence], finance [@menkveld2021non], religion [@hoogeveen2023many], neuroimaging [@botvinik2020variability], political science [@breznau2021observing], machine learning [@chen2024subjectivity], ecology and evolutionary biology [@gould2023same], psychology [@boehm2018estimating, @bastiaansen2020time, @schweinsberg2021same], and medical informatics [@ostropolets2023reproducible], among others.

With little exception, many-analysts studies find that there *is* meaningful variation in both methods and conclusions across researchers. @holzmeister2023heterogeneity finds that researcher variation in design and analysis likely outweighs population variation in effects.

These studies vary considerably, however, in the extent to which they can establish the source of that researcher variation or suggest policies that might reduce it. Establishing that there is variation is important, but is of limited impact if we do not understand why it is there or what we can do about it. Further, many-analysts results may not even imply a problem if not carefully performed. Variation in the original @silberzahn2018many study may be largely explained by the research question not being made sufficiently clear to researchers [@auspurg2021has], and skipping standard meta-analytic practice may overstate variation between researchers by being too sensitive to outlier estimates [@auspurg2023social].

The ability to explain variation between researchers, rather than just show that variation exists, is limited by the size of these many-analyst studies. As will be explored in Section @sec-target-sample, this study pursued a sample of at least 90 researchers so as to have acceptable power to explain differences in variation. @perignon2022reproducibility, in looking at the sources of reproducibility variation using many teams, used a design with 1,000 tests to replicate in order to adequately power comparisons. Since participation in a many-analysts study takes considerable time and effort, sample sizes are often well below even the aforementioned 90, which may explain why many studies do not attempt do decompose the variation in effects they find between sources. These smaller sample sizes can produce acceptable statistical power for some tests but not others, and explaining variation or agreement in effects between researchers generally demands a larger sample than showing the existence of meaningful variation or showing a difference in rates of making a particular research decision. Many-analyst studies that aim to explain the sources of variation between researchers either do so despite the low-power issue, gather larger samples of researchers, or select analyses which produce adequate power despite small samples.

Among studies that do attempt to explain researcher variation, there are three common sources of explanation. The first of these is in the difficulty of the research task, with some studies showing less researcher agreement in more complex or difficult-to-analyze scenarios \[@menkveld2021non, @ortloff2023different,. A second source is researcher experience or characteristics. @menkveld2021non find that higher-quality teams (with more experience, seniority, publishing success, and/or people) agreed more. @ortloff2023different find that experienced researchers tended to draw more abstract codebooks and conclusions than students, and @broderick2020automatic find that replicators with more coding skill found more errors in original work. @breznau2021observing, however, found that researcher characteristics explained only a small share of the variation in results.

A third factor used to explain variation is peer review or evaluation. Seeing the actual impact of review requires that researchers be able to revise their work after receiving it, as in @menkveld2021non, who find that review increases agreement. In some cases there is no chance to revise so we cannot see the impact of peer review, but instead outside evaluation is used as a measure of researcher quality. In this vein, @gould2023same find that peer review scores do not predict whether a given researcher produces an outlier result.

Outside of many-analyst designs, there are studies that use simulation to try many combinations of analytical or data-cleaning choices and examine the resulting variation in estimates. This approach is similar to a many-analysts design in that they look at variability in potential research choices and, often, try to explain variation in effects estimates using those choices. They differ in that they are necessarily limited to the set of research decisions that the project organizers consider ahead of time (which constrains the universe of possible decisions but also makes interpretation of the results far more clear), and typically consider all combinations of decisions equally, rather than favoring combinations an actual researcher would choose. Of these studies, the closest to the present study is @klau2023comparing, a study which evaluates the sensitivity of results in an observational psychological data set to different data preprocessing and modeling choices. They try multiple combinations of reasonably preprocessing and modeling choices using simulation to iterate through the universe of potential choices, and find significant variation in effects over reasonable preprocessing and modeling choices. A similar attempt to separate researcher variation into modeling and preprocessing components is also done in a many-analysts design in @huntington2021influence, although in a limited way.

This study's design attempts to evaluate multiple of these sources of variation using a staged design, similar to @perignon2022reproducibility. The different stages allow different levels of researcher choice along the lines of interpretation of the research question, research design, and data preparation, as well as randomized peer review incorporating these mechanisms proposed by the literature and responding to the critique of @auspurg2021has. Researcher characteristics are collected, as well, allowing for exploration of the researcher-characteristics source of researcher variation, although not in a controlled way. We do not address the difficulty of the research task as a potential source of researcher variation in this study.

# Design

In this study, we attempt to isolate the influence of several different potential sources of researcher variation by having the same set of researchers complete the same research task at least three times. We refer to these main research tasks as Task 1, Task 2, and Task 3. Following each task there is also a round of peer review and an opportunity to revise work.

Task 1 gives each researcher a large amount of freedom in terms of how they plan to complete the research task. Each successive task removes a degree of freedom from the researcher and specifies a specific way that the analysis is to be performed. The intuition behind this design is that if the removal of a specific kind of researcher freedom meaningfully reduces the variation in results between researchees, then that degree of freedom is a meaningful contributor to researcher variation.

The following goals and instructions are shared across all tasks:

-   Estimate the causal effect of a policy on a specified outcome, among the group affected by that policy (see Section \ref{sec:focaltask} below for more details).

-   Use American Community Survey (ACS) data to estimate the effect, using data no older than 2006 and no newer than 2016.

-   Procure ACS data from IPUMS [@ruggles2024ipums], selecting only one-year files and using harmonized variables.

-   Optionally, combine the ACS data with a data set on the presence or absence of other relevant policies, provided by the organizers.

-   Use a statistics package or language that allows results to be immediately replicated.

Researchers were also given background information on the policy itself and its eligibility criteria, guidance on how to use the IPUMS website, instructed to use assistants for any work they would normally use assistants for, and to complete their analysis as though it had been their own idea, rather than attempting to match or not-match other researchers, or asking the project organizers how they would like the analysis to be performed.

These instructions comprise the entirety of the limitations on researchers in Task 2. Tasks 2 and 3 specified the task further and removed researcher degrees of freedom.

-   Task 2 specified the research design more precisely. Instead of allowing any research design to identify the causal effect of interest, Task 2 gave specific definitions for which individuals comprised a "treated" group and which comprised an "untreated" group.[^3] Then, it instructed researchers to estimate the effect by comparing how outcomes for the "treated" group changed from before policy implementation to afterwards against how outcome for the "untreated" group changed. This can be thought of as a difference-in-differences style design, although the phrase "difference-in-differences" was not used in the instructions.

-   Task 3 uses the same research design limitations of Task 2, but also provides a pre-cleaned data set, prepared by the organizers. The data set offered a pre-prepared treated/untreated-group indicator as specified in Task 2, limited the data set only to the treated and untreated group, prepared and cleaned all variables in the data set that did not already come pre-cleaned, handled missing-data flags, merged in state policy data, and offered standardized simplified recodings of demographic variables. Researchers were instructed to not further clean the data or limit the sample.

[^3]: Although eligibility criteria for the policy were explicitly given in Task 1, Task 2 further limits the treated group by narrowing the acceptable age range. The limitation was more impactful for defining the untreated comparison group, though. Many researchers did use a treated/untreated group approach in Task 1 before it was specified in Task 2, but different individuals defined the untreated group in highly diverse ways, as will be shown in the Results section.

Comparison of the researcher output between Task 1 and Task 2 is intended to show the researcher variation introduced by either an imprecise statement of the research question, as in @auspurg2023social, or due to differences in research design choices.

Comparison of the researcher output between Task 2 and Task 3 is intended to show the researcher variation introduced by decisions made in the data cleaning and variable definition process. A researcher following the Task 2 instructions should arrive at the same sample size, number of treated individuals, and number of untreated individuals as in Task 3, as well as the same definition for the outcome variable.[^4] Differences in the data set and in the results between Task 2 and Task 3 should be a result of differences in the data cleaning and preparation process.

[^4]: The Task 2 instructions do leave some leeway for definition of some variables, in particular control variables like education or race, which have a specific recoded version available in Task 3 that are not specified in the Task 2 instructions.

Following each of the research tasks, researchers engage in a round of peer review. 2/3 of researchers are randomly assigned to peer review, and 1/3 do not engage in peer review. Those in peer review are randomly assigned in pairs. Those pairs performed a blind review of each others' work, and provided a written assessment of that work. Reviewers were instructed to produce a review "as though (they) were the reviewer of a journal article," and to judge the work as though they were reviewing for a journal where a study of this kind "could be published if the work was of high quality."

Following peer review, all researchers have an opportunity to revise their work in light of the peer review (or for any other reason). Importantly, revision is not mandatory, nor is satisfying one's peer reviewer, and the majority researchers did not choose to submit revisions.

Notably, this form of peer review does not exactly match what is typically done in peer review work for journal publications. In particular, revision is non-mandatory, all reviewers have themselves completed a study with the same goal and data and so have extensive background information, and all reviewers are themselves also reviewed by the same person. These features will all affect interpretation of the peer review results. In particular, the non-mandatory nature of the peer review means that the between-round revision work is only visible for a small subset of the researchers, and the paired nature of the reviews means we cannot separate the effect of being reviewed from the effect of reviewing someone else.

Task 1 gives each researcher a large amount of freedom in terms of how they plan to complete the research task. Each successive task removes a degree of freedom from the researcher and specifies a specific way that the analysis is to be performed. The intuition behind this design is that if the removal of a specific kind of researcher freedom meaningfully reduces the variation in results between researchees, then that degree of freedom is a meaningful contributor to researcher variation.

The following goals and instructions are shared across all tasks:

-   Estimate the causal effect of a policy on a specified outcome, among the group affected by that policy (see Section \ref{sec:focaltask} below for more details).

-   Use American Community Survey (ACS) data to estimate the effect, using data no older than 2006 and no newer than 2016.

-   Procure ACS data from IPUMS [@ruggles2024ipums], selecting only one-year files and using harmonized variables.

-   Optionally, combine the ACS data with a data set on the presence or absence of other relevant policies, provided by the organizers.

-   Use a statistics package or language that allows results to be immediately replicated.

Researchers were also given background information on the policy itself and its eligibility criteria, guidance on how to use the IPUMS website, instructed to use assistants for any work they would normally use assistants for, and to complete their analysis as though it had been their own idea, rather than attempting to match or not-match other researchers, or asking the project organizers how they would like the analysis to be performed.

These instructions comprise the entirety of the limitations on researchers in Task 2. Tasks 2 and 3 specified the task further and removed researcher degrees of freedom.

-   Task 2 specified the research design more precisely. Instead of allowing any research design to identify the causal effect of interest, Task 2 gave specific definitions for which individuals comprised a "treated" group and which comprised an "untreated" group.\^\[Although eligibility criteria for the policy were explicitly given in Task 1, Task 2 further limits the treated group by narrowing the acceptable age range. The limitation was more impactful for defining the untreated comparison group, though. Many researchers did use a treated/untreated group approach in Task 1 before it was specified in Task 2, but different individuals defined the untreated group in highly diverse ways, as will be shown in the Results section.\] Then, it instructed researchers to estimate the effect by comparing how outcomes for the "treated" group changed from before policy implementation to afterwards against how outcome for the "untreated" group changed. This can be thought of as a difference-in-differences style design, although the phrase "difference-in-differences" was not used in the instructions.

-   Task 3 uses the same research design limitations of Task 2, but also provides a pre-cleaned data set, prepared by the organizers. The data set offered a pre-prepared treated/untreated-group indicator as specified in Task 2, limited the data set only to the treated and untreated group, prepared and cleaned all variables in the data set that did not already come pre-cleaned, handled missing-data flags, merged in state policy data, and offered standardized simplified recodings of demographic variables. Researchers were instructed to not further clean the data or limit the sample.

Comparison of the researcher output between Task 1 and Task 2 is intended to show the researcher variation introduced by either an imprecise statement of the research question, as in @auspurg2023social, or due to differences in research design choices.

Comparison of the researcher output between Task 2 and Task 3 is intended to show the researcher variation introduced by decisions made in the data cleaning and variable definition process. A researcher following the Task 2 instructions should arrive at the same sample size, number of treated individuals, and number of untreated individuals as in Task 3, as well as the same definition for the outcome variable.\^\[The Task 2 instructions do leave some leeway for definition of some variables, in particular control variables like education or race, which have a specific recoded version available in Task 3 that are not specified in the Task 2 instructions.\] Differences in the data set and in the results between Task 2 and Task 3 should be a result of differences in the data cleaning and preparation process.

Following each of the research tasks, researchers engage in a round of peer review. 2/3 of researchers are randomly assigned to peer review, and 1/3 do not engage in peer review. Those in peer review are randomly assigned in pairs. Those pairs performed a blind review of each others' work, and provided a written assessment of that work. Reviewers were instructed to produce a review "as though (they) were the reviewer of a journal article," and to judge the work as though they were reviewing for a journal where a study of this kind "could be published if the work was of high quality."

Following peer review, all researchers have an opportunity to revise their work in light of the peer review (or for any other reason). Importantly, revision is not mandatory, nor is satisfying one's peer reviewer, and the majority researchers did not choose to submit revisions. The non-mandatory nature of revision will impact the interpretation of peer review results.

Following each research task and revision, researchers filled out a survey about their work.[^5] This survey asked them to report their findings, additional information like sample size and standard errors, and choices made in the process of doing the analysis like sample restrictions, treated-group definitions, estimator, and standard error adjustments. Researchers were also asked to justify why they had made these choices.

[^5]: Note that the design of this study, and this survey, predates @sarafoglou2024subjective and so does not follow it.

This research design and analysis plan has been preregistered [@portner_huntington-klein_2022]. Analyses that were not preregistered will be noted in the results section as they are performed. Full instructions for each task, as well as post-task survey text and the peer-reviewing instructions, are available in the online appendix.

# Data

### The Focal Research Task

\label{sec:focaltask}

In all research tasks, the specific goal given to researchers was:[^6]

[^6]: Full instructions are available in the online appendix.

> Among ethnically Hispanic-Mexican Mexican-born people living in the United States, what was the causal impact of eligibility for the Deferred Action for Childhood Arrivals (DACA) program (treatment) on the probability that the eligible person is employed full-time (outcome), defined as usually working 35 hours per week or more?
>
> DACA was implemented in 2012. Examine the effects on full-time employment in the years 2013-2016.

In simple terms, this asks researchers to estimate the impact of the DACA program on the probability that those eligible for the program usually work 35 hours per week or more in the years 2013-2016.[^7]

[^7]: Notably, there are several existing papers that use the same ACS data set to identify the effect of DACA on various outcomes. The design used in Tasks 2 and 3 was most directly inspired by @amuedo2016can, although the designs do not match exactly, and the outcomes of interest are not the same. Researchers are informed that such previous studies exist and that they can optionally look into previous studies for background as they would normally do when performing research, although no specific previous study is listed. The instructions emphasize that any previous study should not be understood to be a "right answer" that researchers should be trying to match.

Researchers, many of whom are not from the United States and so may not be familiar with DACA, are given further background information about the DACA program:

-   DACA allowed undocumented immigrants who were accepted into the program to have legal work authorization for two years without fear of deportation, and also allowed them to apply for drivers' licenses or other forms of identification. People could reapply after the two years expired, and many did.

-   Applications for the program opened on August 15, 2012, and over the first four years of the program's existence, over 900,000 applications were received, about 90% of which were approved.[@citservices2016]

-   While the program was not specific to immigrants from any origin country, because of the structure of undocumented immigration to the United States, the great majority of eligible people were from Mexico.

Researchers were also given information on the eligibility criteria for DACA, which was intended to apply only to a specific subset of undocumented immigants who arrived in the United States as children, and not to all undocumented immigrants. Eligible people must:

-   Have arrived in the United States before their 16th birthday.

-   Describe the research task

-   Not have had their 31st birthday as of June 15, 2012.

-   Have lived continuously in the United States since June 15, 2007.

-   Were present in the United States on June 15, 2012 and did not yet have legal status (either citizenship or legal residency) during that time.

An additional eligibility requirement was mistakenly omitted from the Task 1 instructions, but was included for Tasks 2 and 3:

-   Eligible people must have completed at least high school (12th grade) or be a veteran of the military.

In addition to this information about the policy itself and the effect that researchers are supposed to identify, researchers were also given instructions about the data set to use and how to procure it, as well as some details on usage of the data:

-   Data should come from the American Community Survey (ACS), using data no older than 2006, and no newer than 2016.

-   In addition, a file of state/year-level data was provided including labor market data and the presence or absence of different immigration policies in different years. Immigration policy data comes from @urbaninstdata.[^8]

    ACS data should be procured from the IPUMS website [@ruggles2024ipums], specifically selecting one-year ACS files and harmonized variables. Written and video instructions were included showing how to select data samples and variables on the IPUMS website.

-   Researchers were not told which specific variables to use to determine eligibility status, but they were given guidance onto how to find relevant vairables (like looking at the Person $\rightarrow$ Race, Ethnicity, and Nativity page to find variables relevant to ethnicity, birthplace, citizenship, and year of immigration).

-   Several relevant features of the ACS that may affect analysis were emphasized: (a) ACS is a repeated cross-section, not a year-to-year panel data set, and (b) ACS does not list the month that data was collected in, so it is not possible to distinguish whether a given observation in 2012 is from before or after the policy was implemented, and (c) we do not actually observe in ACS whether a given person is enrolled in DACA, so we assume that all eligible people who are ethnically Mexican and Mexican-born are treated.

[^8]: This file included the state/year-level unemployment rate and labor force participation rate. Immigration policy flags were for policies for undocumented immigrants to get state drivers' licenses, to get college financial aid, to be banned from state public colleges, or to follow Omnibus immigation legislation that serves to increase the surveillance of immigation documentation. Additional indicators were for participation in E-Verify laws that require employers to verify immigration authorization, to limit E-Verify participation, participation in Secure Communities, and for participation in task-force or jail based 287(g) policies.

Finally, researchers were instructed to keep track of any variables used to limit their sample download on IPUMS, and to review the survey where they would be reporting their results before beginning their analysis.

From there, researchers were given free reign to complete the analysis as they thought most appropriate, including their own choice of statistical software, an instruction to use assistants for any work that they might normally use assistants for, and asking them to complete the analysis as they thought best, as though the research task had been their own idea, not trying to match or not-match other researchers or guess what analyses the project organizers wanted to see. Once finished, they uploaded all of their code and data to a Sharepoint website, wrote a short description and interpretation of their results focusing on a single "headline" result, and filled out the research survey to report their results.

For Task 2, all of the previous instructions remained in place, but several were added to further specify the research design:

-   There is a "treated" group that is comprised of all ethnically Mexican and Mexican-born individuals who are aged 26-30 on June 15, 2012 (recall that individuals must not have had their 31st birthday as of June 15, 2012 to be eligible for DACA).

-   There is an "untreated" group that is comprised of people who would have been eligible for DACA, except that they were aged 31-35 on June 15, 2012.

-   Researchers should estimate the effect of treatment by seeing how the 26-30 group changed from before treatment to after relative to how the 31-35 group changed (keeping in mind this is a repeated cross-section and not panel data).

-   Researchers should attempt to estimate the effect for all individuals in the "treated" group and not, for example, estimate the effect only for men or only for women.

-   The instructions specifically mention that researchers can, if they like, use covariates or account for differing trends to improve the comparability of the treated and untreated groups.

The task is otherwise unchanged for Task 2.

In Task 3, the instructions remain unchanged from Task 2, except that the data is provided directly instead of having researchers download data from IPUMS, omitting data from the year of 2012. In Task 3, project organizers cleaned the data, merged in the state policy data, created a variable indiciating whether a given individual was in the "treated" or "untreated" group, limited the sample only to individuals in "treated" or "untreated," and created simplified versions of variables like education. Researchers were instructed not to further limit the sample from this prepared data set, or to perform further extensive data cleaning.[^9]

[^9]: There were three observations in the final cleaned data set that were missing values of the education variable. The final used sample in Task 3 sometimes differs by 3 across researchers, based on whether the analysis uses education and thus drops these individuals.

### Recruitment and Attrition

In a many-analysts study, researchers who carry out the research task make up both the bulk of the author list and are the subject of inquiry, so their recruitment is a key feature of the study.

#### Researcher Qualifications

The goal of the project organizers was to make the set of researchers representative of the set of people who are producing the applied microeconomics literature. As such, recruitment criteria focused on identifying people who have produced applied microeconomic research, including potentially non-academic applied microeconomics research.

A given researcher was qualified for the project if they satisfied any one of the following criteria:

-   They are academic faculty working in applied microeconomics.

-   They are a graduate student **and** have a published or forthcoming paper in applied microeconomics.

-   They hold a PhD **and** work in a job where they write non-academic reports using tools from applied microeconomics to estimate causal effects.[^10]

[^10]: This qualification would allow, for example, employees of the World Bank, or people working in private sector research, to participate.

Participation was not limited on the basis of country, career stage, or demographics such as sex, race, or sexual or gender identity.

#### Target Sample Size

\label{sec-target-sample}

An initial simulation-based power analysis assumed that each research task would have 5% less between-researcher variation in observed effects than the previous round and looked at the statisical power to detect a linear relationship between round number and the squared deviation of effects (variance of estimated effects across researchers). We found that we had 90% power to detect this effect if 90 researchers finished all tasks. We also found that, for comparisons of only two different research tasks, 90 researchers would give 85% power to detect a decline in variance from one stage to the next of 15% or more, a reasonable effect size given previous many-analyst studies.

We further assumed that attrition rates would be roughly 50%, which would suggest recruiting 180 eligible researchers to achieve adequate power. We revised that goal to 200 to account for our assumptions potentially being optimistic. Project organizers obtained funding to support payments to 200 researchers (see below).

#### Recruitment and Incentives

Recruitment was advertised to potential researchers through three avenues: (1) social media posts on Twitter and LinkedIn, (2) emails to professional organizations including the Institute for Replication and the Committee on the Status of Women in the Economics Profession, and (3) emails to United States economics department chairs. For emails to departments heads, we gathered the list of all 286 economics departments listed in the U.S. News and World Report. We could locate emails for a front desk or (preferably) department chair for 264 of those departments. We emailed those 264 departments, asking for the message to be passed on to all faculty or just all microeconomics faculty.

The recruitment message described the project and its goals, and provided a link to a website that included further detail on project expectations and incentives for participation.[^11] Researchers were told that if they completed all stages of the project, they would be offered authorship on the eventual paper and a \$2,000 payment for up to 200 of the participants. The website included a link to a survey that asked questions related to eligibility for the project.

[^11]: <https://nickch-k.github.io/ManyEconomists/>

#### Participation and Attrition

```{r}
source('../code/participation_and_attrition.R')
```

Overall participation and attrition values are in Table \ref{tbl-attrition}. `r justcount[1,Participants]` people submitted applications for the project. `r justcount[1, Attrition]` of these were found to be ineligible for the project. Most of these were graduate students who did not yet have a forthcoming paper.

```{r attrition}
#| label: tbl-attrition
#| tbl-cap: Participation and Attrition
justcount |> knitr::kable(booktabs = TRUE)
```

This left `r justcount[2,Participants]` eligible participants. This is more than the 200 for which budget was available to pay the offered \$2,000 incentive. The 282 of these participants who had signed up by the original cutoff date were put into a random order, and then the 13 late signups were put at the end of this order. Participants were given their place in the order, and informed that, among people completing all stages of the project, the first 200 in the order would be paid.

Initial assumptions from the power analysis that attrition rates would be near 50% were almost exactly correct, with `r percent(justcount[5,Participants]/justcount[2,Participants],.01)` of these initial `r justcount[2,Participants]` eligible researchers completing all three stages. Nearly all of the attrition occurred by the completion of Task 1. After `r justcount[3,Participants]-justcount[2,Participants]` eligible researchers failed to complete Task 1, only a further `r justcount[5,Participants]-justcount[3,Participants]` failed to complete Task 3. This means we have `r justcount[5,Participants]` researchers who completed all three research tasks, well above the goal of 90.

The high recruitment numbers and the fact that nearly all attrition occurs before Task 1 is complete allows us to evaluate the impact of the payment incentive. One potential concern with our incentive design is that payment and authorship are offered to anyone who completes all tasks, regardless of the quality of their work. We evaluate whether being guaranteed payment affects the probability of completing Task 1 using a regression discontinuity design. Someone randomly assigned to position 199 in the ordering is guaranteed payment if they complete all the tasks, while someone in position 201 may think they are likely to receive payment, but they are not guaranteed it.

```{r}
#| label: fig-rdd
#| fig-cap: Impact of Guaranteed Payment on Probability of Task 1 Completion

rdplot(moneyatt$Finished,moneyatt$order,200,
       x.label = 'Order',y.label = 'Prob. Completed First Task',
       title = '')
```

Figure \ref{fig-rdd} shows no meaningful effect of being guaranteed payment on the probability of completing Task 1. In additional results in the appendix, using a linear regression specification of the regression discontinuity design and the full range of the data (not including the late sign-ups) to maximize statistical power,[^12] we again find no statistically significant effect of being guaranteed treatment. This is suggestive that participants were not simply signing up in an attempt to get a \$2,000 payment for little effort.

[^12]: Use of the full range, rather than a bandwidth, is justified given that the running variable is randomly assigned.

#### Sample Characteristics

Tables \ref{tab-samp1} to \ref{tab-samp3} show the characteristics of the recruited sample, and how those characteristics changed with eligibility and attrition. Task 2 is omitted as an attrition stage since so few people dropped out between Task 1 and Task 2.

Table \ref{tab-samp1} shows that the majority of researchers were recruited via social media, with only about 9% coming from a department email, 4% from a professional organization email, and 9% from some other source (like word-of-mouth). Those recruited from another source were less likely to qualify for the study, and slightly less likely to finish, while those recruited from social media were most likely to qualify and finish. We also asked researchers how certain they were of their ability to finish the first task as well as the full set of tasks, on a scale of 1 to 100. Enrollees were about 90% confident in their ability to complete the full set of research tasks (although only about 50% did). Those who were more confident were slightly more likely to actually finish, and average confidence rates of those who did finish were about 92% instead of 90%.

```{r}
#| output: asis
sumtable(alldemog, vars = c('Researcher_Q11',
                            'Researcher_Q12_1',
                            'Researcher_Q12_2'), 
         labels = c('Recruitment Source','Certainty to Finish Task 1','Certainty to Finish Task 3'), group = 'Round',
         out = 'latex',
         title = 'Researcher Recruitment Source and Completion Confidence',
         fit.page = '\\textwidth',
         anchor = 'tab-samp1')
```

Table \ref{tab-samp1} shows the professional experience of enrollees. While graduate students were considered eligible for the project as long as they had a published or forthcoming paper, the great majority of eligible researchers (`r alldemog[Round == 'Assigned task 1',percent(mean(Researcher_Q10 %in% c('PhD','Prof. Degree')),1)]`) had PhDs. PhD holders were also more likely than other eligible researchers to complete all three tasks.

These PhDs are split across faculty (`r alldemog[Round == 'Assigned task 1',percent(mean(Researcher_Q6 %in% c('Faculty')),1)]`) and other non-faculty researchers (`r alldemog[Round == 'Assigned task 1',percent(mean(Researcher_Q6 %in% c('Other Researcher')),1)]`), both of which were more likely than graduate students to finish all three rounds. Note that the researchers in these categories who do not hold PhDs were either people who had been hired to faculty roles without holding PhDs (such as ABDs, or people in countries where a faculty position requires only a Master's degree), or people with Master's degrees in non-faculty research positions who had published academic papers (some of whom were still graduate students).

Most of the researchers had at least one published paper, and researchers with 6+ papers were more likely than others to complete all three research tasks. Those with "No Academic Papers" are non-academic researchers who produce work not intended for academic journal publication. Those with "No Published Academic Papers" have papers that are forthcoming, or are faculty who only have working papers and no publications.

The set of researchers in the study generally do not work in the specific subfield that the research task is in. The research task is similar to many studies done across all of applied microeconomics, but specifically is on the topics of labor and immigration. About a third of the enrollees had done research in either immigration or labor previously, and these researchers were somewhat more likely to complete all three tasks. No researchers enrolled who had previously worked in both immigration and labor.

```{r}
#| output: asis
sumtable(alldemog, vars = c('Researcher_Q10','Researcher_Q6', 'Q8Recode','Researcher_Cats'), 
         labels = c('Degree','Occupation', 'Research Experience','Field'), group = 'Round',
         out = 'latex', fit.page = '\\textwidth',
         title = 'Researcher Professional Experience',
         anchor = 'tab-samp2')
```

Table \ref{tab-samp3} shows the demographics of the researcher sample. The eligible sample was just under 80% male and more than 55% white, and both percentages grew by the conclusion of task 3, with the white share growing significantly to 66%. The 80% male figure is similar to the share male found for faculty at a selected set of top economics departments in 2017 by @lundberg2019women, and among all actively publishing economists in 2019 by @card2022gender. A small share reported being LGBTQ+, and this share remained constant over all rounds of the research tasks. An additional form of demographic difference is geographic. About half of the sample was situated in the United States, and about half was from another country.[^13] The representativeness of the racial mixture is difficult to assess for this reason; 66% white would be low if the entire sample were from the United States [@stansbury2023economics], but it is unclear what the population rate is in a 50% US/50% other location sample.

[^13]: Exact figures are not given for geography, and crosstabulations across geography are not given, because non-geographic demographic information comes from a survey where we acquired permission to share aggregate figures. Geographic information, on the other hand, comes from researcher payments information, for which we did not request permission to share responses.

```{r}
#| output: asis
sumtable(alldemog, vars = c('Researcher_Q15',
                            'RaceRecode',
                            'Researcher_Q17'), 
         labels = c('Gender','Race','LGBTQ+'), group = 'Round',
         out = 'latex',
         title = 'Researcher Demographics',
         fit.page = '\\textwidth',
         anchor = 'tab-samp3')
```

```{r}
# OMIT THIS TABLE, BUT FROM IT WE GET 1 completed Python, 1 SPSS, 1 R/Stata, 33 R, 109 Stata in completed
# alldemog[Q2 == 'Finished task 3'] |>
#   sumtable(vars =  c('Researcher_Q10','Researcher_Q6', 'Q8Recode','Researcher_Q15',
#                             'RaceRecode',
#                             'Researcher_Q17',
#                      'Researcher_Q11',
#                      'Researcher_Cats',
#                      'Language'), 
#          labels = c('Degree','Occupation', 'Research Experience','Gender','Race','LGBTQ+','Recruitment Source','Field',
#                     'Coding Language'),
#          col.breaks = 4,
#          out = 'latex',
#          fit.page = '\\textwidth',
#          anchor = 'tab-samp4')

```

One researcher did complete all three research tasks, and appears in the above tables, but their work has been removed from the results that follow in the rest of the paper, as due to a misunderstanding of the instructions, their work did not attempt to estimate the effect of DACA on the probability of employment.

As a whole, the goal of constructing a sample that largely reflects the group of people who publish work in applied microeconomics. The sample is skewed towards the United States, which is partially driven by the emails sent to US economics departments, the fact that the project was advertised and carried out in English, and the fact that the project organizers are in the United States and advertised the project using their own social media. Given that caveat, the makeup of the sample appears to be fairly similar to the makeup of the profession itself, although this is difficult to verify for some demographics.

# Results

(move these around as appropriate; the recruitment and attrition stuff might move up to data)

## Variation in Effects and Sample Sizes

```{r}
source('../code/variation_in_effects_and_sample_sizes.R')

p_effect_distribution
```

```{r}
p_full_effect_distribution_individual
```

```{r}
p_sample_size_distributions
```

```{r}
p_treated_group_sample_size
```

As a whole, the goal of constructing a sample that largely reflects the group of people who publish work in applied microeconomics. The sample is skewed towards the United States, which is partially driven by the emails sent to US economics departments, the fact that the project was advertised and carried out in English, and the fact that the project organizers are in the United States and advertised the project using their own social media. Given that caveat, the makeup of the sample appears to be fairly similar to the makeup of the profession itself, although this is difficult to verify for some demographics.

# Results

(move these around as appropriate; the recruitment and attrition stuff might move up to data)

## Researcher Characteristics and Effects

```{r}
source('../code/researcher_characteristics_and_effects.R')

res_tab_effect |>
  knitr::kable(booktabs = TRUE)
```

```{r}
res_tab_deviation |>
  knitr::kable(booktabs = TRUE)
```

```{r}
#| fig-width: 8
#| fig-height: 6
p_deviations_by_language

# NOTE FOR WRITEUP: Of the big R outliers two people were in that category every round, and everyone else was only in there once.
```

## Peer Review

```{r}
source('../code/peer_review.R')
p_peer_review_effect_distributions
```

### Do You Become More Like Your Reviewer?

```{r}
#| fig-width: 8
#| fig-height: 5
p_more_like_reviewer
```

```{r}
peer_review_reg |> 
  msummary(stars = c('*' = .1, '**' = .05, '***' = .01),
                         gof_omit = c('IC|R2|RMSE|Err'),
           coef_rename = c('Intercept',
                           'Comparison: Next Round',
                           'Comparison: Next Round vs. This Round',
                           'Unreviewed',
                           'Next Round x Unreviewed',
                           'Next vs. This x Unreviewed'),
           output = 'kableExtra') 
```

TO DO: The same but for sample sizes and analytic choices

## Analytic Choices

```{r}
source('../code/analytic_choices.R')

base %>%
  mutate(se_adjustment = factor(se_adjustment, levels = c(
    'Cluster (State)',
    'Cluster (State & Year)',
    'Cluster (ID/Strata/Other)',
    'Het-Robust',
    'Other/Bootstrap',
    'None'
  ))) %>%
  filter(!(round %like% 'Revision')) %>%
  sumtable(vars = c('method','Weights','se_adjustment'), 
         labels = c("Method",'Weights','S.E. Adjustment'),
         title = "",
         col.breaks = 2,
         note = 'This table shows details on estimation, not research design. "Difference-in-differences" implemented with linear regression, for example, counts here as linear regression.')
```

## Sample Limitations

```{r}
source('../code/sample_limitations.R')
# Basic sample limitations table
sumtable(basic_samp_limitations, vars = c('Whole Sample','Treated Group','Untreated Group'), add.median = TRUE, group = 'Round', group.long = TRUE)
```

```{r}
#| output: asis
# Variables used in limitations
sumtable(r12samp, titles, group = 'Round/Sample',
         out = 'latex',
         fit.page = '.9\\textwidth',
         title = 'Sample Restriction Methods')
```

```{r}
#| output: asis
# sample limitationsand effects/samples, task 1
cat(dftoLaTeX(make_efftab(r12samp[Round == 'Task 1']),
          align = 'llllllllll',
            title = 'Task 1 Effect and Samples by Sample Definitions',
            anchor = 'tab:task1effectsample',
            fit.page = '\\textwidth'))
```

```{r}
#| output: asis
# sample limitationsand effects/samples, task 2
cat(dftoLaTeX(make_efftab(r12samp[Round == 'Task 2']),
          align = 'llllllllll',
            title = 'Task 2 Effect and Samples by Sample Definitions',
            anchor = 'tab:task2effectsample',
            fit.page = '\\textwidth'))
```

## Control Variables

```{r}
source('../code/clean_controls.R')

allcontrols[, .(N = uniqueN(paste0(Q1,Round)),
                Effect = number(mean(Effect, na.rm = TRUE), .001),
                `Mean SE` = number(mean(SE, na.rm = TRUE), .001),
                `Effect SD` = number(sd(Effect, na.rm = TRUE), .001)),
            by = Control][order(-Effect)] |>
  knitr::kable(booktabs = TRUE)


```

```{r}

trans_con[, .(N = uniqueN(paste0(Q1,Round)),
                Effect = number(mean(Effect, na.rm = TRUE), .001),
                `Mean SE` = number(mean(SE, na.rm = TRUE), .001),
                `Effect SD` = number(sd(Effect, na.rm = TRUE), .001)),
            by = .(Category = category, Control = Relabel)][order(Control)] |>
  knitr::kable(booktabs = TRUE)
```

```{r}
# Average appearances across rounds
allcontrols[, .(Average = number(.N/first(Total), .01)), by = .(Control = factor(Control, levels = controllevs), Round)] |>
  dcast(Control ~ Round, value.var = 'Average') |>
  knitr::kable(booktabs = TRUE)

```

## Round 2 Bimodality

```{r}
source('../code/round_2_bimodality.R')
p_sample_size_scatter
```

```{r}
p_standard_error_scatter
```

```{r}
p_task1_vs_task2
```

```{r}
shtask |>
  knitr::kable(booktabs = TRUE)

```

```{r}
conchange[, .(Increase = percent(mean(R2Effect > .05, na.rm = TRUE))),by = .(Control, ControlSwitch)] |>
  dcast(Control ~ ControlSwitch) |>
  knitr::kable(booktabs = TRUE)
```

```{r}
conchange[, .(N = .N),by = .(Control, ControlSwitch)] |>
  dcast(Control ~ ControlSwitch)
```

```{r}
numberp1 = label_number(accuracy = .01)
changesamp[, .(N = .N, Increase = numberp1(mean(Increase, na.rm = TRUE)),
               `Standard Error` = numberp1(median(`Standard Error`, na.rm = TRUE)),
               R2Effect = numberp1(mean(R2Effect, na.rm = TRUE)),
               Abovep05 = percent(mean(R2Effect > .05, na.rm = TRUE), .1)),
           by = .(`Sample Limitation`, Changed)][order(`Sample Limitation`, Changed)][!is.na(Changed)] |>
  knitr::kable(booktabs = TRUE)
```

```{r}
p_match_vs_mismatch_distribution
```

```{r}
p_distribution_by_instruction_match
```

```{r}
rfield |>
  knitr::kable(booktabs = TRUE)
```

# Conclusion

## Recommendations for Improved Practice

-   How we think this means people should change their research processes

-   Possibilities:

    -   Data cleaning best practices

    -   Transparency about the data cleaning and preparation process in publications

    -   Inclusion of data cleaning and preparation code in replication practices

    -   Treatment of sample selection in a similar robustness- or multiverse analysis-style way to how analytic choices are treated

## Discussion

-   Clearly a lot of different choices are made

-   But we actually get a fair amount of agreement here, and the effects themselves don't vary *that* much

-   Note this suggests a fairly standard design that lots of microeconomists would be familiar with

-   Differences in peer review findings

-   THe things on which we have standards and common practice, we use them. On the things we don't, we don't. This should be recognized.

-   Implications for reading empirical results
